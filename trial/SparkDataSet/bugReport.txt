MLLIB Word2Vec wordVectors divided by Euclidean Norm equals to zero	In Word2VecModel, wordVecNorms may contains Euclidean Norm equals to zero. This will cause incorrect calculation for cosine distance. when you do cosineVec(ind) / wordVecNorms(ind). Cosine distance should be equal to 0 for norm = 0.
Decision tree binary classification with ordered categorical features: incorrect centroid	In DecisionTree and RandomForest binary classification with ordered categorical features, we order categories' bins based on the hard prediction, but we should use the soft prediction.
Pyspark - DataFrame - Optional Metadata with `None` triggers cryptic failure	If the optional metadata passed to `pyspark.sql.types.StructField` includes a pythonic `None`, the`pyspark.SparkContext.createDataFrame` will fail with a very cryptic/unhelpful error. # Assumes sc exists import pyspark.sql.types as typessqlContext = SQLContext(sc)literal_metadata = types.StructType([    types.StructField(        'name',        types.StringType(),        nullable=True,        metadata={'comment': 'From accounting system.'}        ),    types.StructField(        'age',        types.IntegerType(),        nullable=True,        metadata={'comment': None}        ),    ])literal_rdd = sc.parallelize([    ['Bob', 34],    ['Dan', 42],    ])print(literal_rdd.take(2))failed_dataframe = sqlContext.createDataFrame(    literal_rdd,    literal_metadata,    )Traceback (most recent call last):  File "<stdin>", line 1, in <module>  File "<string>", line 28, in <module>  File "S:\ZQL\Software\Hotware\spark-1.5.0-bin-hadoop2.6\python\pyspark\sql\context.py", line 408, in createDataFrame    jdf = self._ssql_ctx.applySchemaToPythonRDD(jrdd.rdd(), schema.json())  File "S:\ZQL\Software\Hotware\spark-1.5.0-bin-hadoop2.6\python\lib\py4j-0.8.2.1-src.zip\py4j\java_gateway.py", line 538, in __call__  File "S:\ZQL\Software\Hotware\spark-1.5.0-bin-hadoop2.6\python\pyspark\sql\utils.py", line 36, in deco    return f(*a, **kw)  File "S:\ZQL\Software\Hotware\spark-1.5.0-bin-hadoop2.6\python\lib\py4j-0.8.2.1-src.zip\py4j\protocol.py", line 300, in get_return_valuepy4j.protocol.Py4JJavaError: An error occurred while calling o757.applySchemaToPythonRDD.: java.lang.RuntimeException: Do not support type class scala.Tuple2. at org.apache.spark.sql.types.Metadata$$anonfun$fromJObject$1.apply(Metadata.scala:160) at org.apache.spark.sql.types.Metadata$$anonfun$fromJObject$1.apply(Metadata.scala:127) at scala.collection.immutable.List.foreach(List.scala:318) at org.apache.spark.sql.types.Metadata$.fromJObject(Metadata.scala:127) at org.apache.spark.sql.types.DataType$.org$apache$spark$sql$types$DataType$$parseStructField(DataType.scala:173) at org.apache.spark.sql.types.DataType$$anonfun$parseDataType$1.apply(DataType.scala:148) at org.apache.spark.sql.types.DataType$$anonfun$parseDataType$1.apply(DataType.scala:148) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.immutable.List.foreach(List.scala:318) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at scala.collection.AbstractTraversable.map(Traversable.scala:105) at org.apache.spark.sql.types.DataType$.parseDataType(DataType.scala:148) at org.apache.spark.sql.types.DataType$.fromJson(DataType.scala:96) at org.apache.spark.sql.SQLContext.parseDataType(SQLContext.scala:961) at org.apache.spark.sql.SQLContext.applySchemaToPythonRDD(SQLContext.scala:970) at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) at java.lang.reflect.Method.invoke(Unknown Source) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379) at py4j.Gateway.invoke(Gateway.java:259) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Unknown Source)
Spark SQL CLI will set sessionstate twice	In SparkSQLCLI, we have created a CliSessionState, but then we call SparkSQLEnv.init(), which will start another SessionState. This would lead to exception because processCmd need to get the CliSessionState instance by calling SessionState.get(), but the return value would be a instance of SessionState.
GaussianMixture.train crashes if an initial model is not None	Steps to reproduce :from pyspark.mllib.clustering import GaussianMixture from numpy import arrayreportdata = sc.textFile("data/mllib/gmm_data.txt")reportparsedData = data.map(lambda line: array([float(x) for x in line.strip().split(' ')])) gmm = GaussianMixture.train(parsedData, 2)reportGaussianMixture.train(parsedData, 2, initialModel=gmm) It looks like the source of the problem is initialModelWeights NumPy array. In 1.5 / 1.6 it leads to net.razorvine.pickle.PickleException, in 1.4 we get Method trainGaussianMixture([..., class org.apache.spark.mllib.linalg.DenseVector, class java.util.ArrayList, class java.util.ArrayList]) does not existreport
ChiSqTest gets slower and slower over time when number of features is large	I've been running a ChiSqTest to pick features for feature reduction. My understanding is that internally it creates jobs to run on batches of 1000 features at a time.I was under the impression that the features are treated as independant, but this does not appear to be the case. When the number of features is large (160k in my case), each batch gets slower and slower. As an example, running on 25 m3.2xlarges on Amazon EMR, it started at just over 1 minute per batch. By the end, batches were taking over 30 minutes per batch
pyspark shell uses execfile which breaks python3 compatibility	The pyspark shell allows custom start scripts to run using the PYTHONSTARTUP environment variable. The value specified there will get run at the end of the shell startup by a call to execfile. However, execfile is deprecated in python3 and thus this does not work for python3 users. 
Fix schema inferance on local collections	Current schema inferance for local python collections halts as soon as there are no NullTypes. This is different than when we specify a sampling ratio of 1.0 on a distributed collection. This could result in incomplete schema information.
Stack overflow with endless call of `Delegation token thread` when application end.	When application end, AM will clean the staging dir.But if the driver trigger to update the delegation token, it will can't find the right token file and then it will endless cycle call the method 'updateCredentialsIfRequired'.Then it lead to StackOverflowError.
GLM summary crashes with NoSuchElementException if attributes are missing names	In getModelFeatures() of SparkRWrappers.scala, we call _.name.get on all the feature column attributes. This fails when the attribute name is not defined.One way of reproducing this is to perform glm() in R with a vector-type input feature that lacks ML attrs, then trying to call summary() on it, for example:df <- sql(sqlContext, "SELECT * FROM testData")df2 <- withColumnRenamed(df, "f1", "f2") // This drops the ML attrs from f1lrModel <- glm(hours_per_week ~ f2, data = df2, family = "gaussian")summary(lrModel) // NoSuchElementException
Display correct error message when accessing REST API with an unknown app Id	I got an exception when accessing the below REST API with an unknown application Id.http://<server-url>:18080/api/v1/applications/xxx/jobsInstead of an exception, I expect an error message "no such app: xxx" which is a similar error message when I access "/api/v1/applications/xxx"
The implementation of ParamMap#filter is wrong	ParamMap#filter uses `mutable.Map#filterKeys`. The return type of `filterKey` is collection.Map, not mutable.Map but the result is casted to mutable.Map using `asInstanceOf` so we get `ClassCastException`. Also, the return type of Map#filterKeys is not Serializable.
Incorrect calculation of row size in o.a.s.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoiner	"I noticed that the row size is incorrectly calculated.The ""sizeReduction"" value is calculated in words:// The number of words we can reduce when we concat two rows together.// The only reduction comes from merging the bitset portion of the two rows, saving 1 word.val sizeReduction = bitset1Words + bitset2Words -outputBitsetWords but then it is subtracted from the size of the row in bytes:out.pointTo(buf, $ {schema1.size + schema2.size}, sizeInBytes - $sizeReduction);"
[SQL] Tungsten projection fails for null values in array fields	Accessing null elements in an array field fails when tungsten is enabled.It works in Spark 1.3.1, and in Spark > 1.5 with Tungsten disabled.With Tungsten enabled, it reaches NullPointerException.
Executors are not always terminated successfully by the worker	There are cases when the executor is not killed successfully by the worker.One way this can happen is if the executor is in a bad state, fails to heartbeat and the master tells the worker to kill the executor. The executor is in such a bad state that the kill request is ignored. This seems to be able to happen if the executor is in heavy GC.The cause of this is that the Process.destroy() API is not forceful enough. In Java8, a new API, destroyForcibly() was added. We should use that if available.
make_distribution should not override MAVEN_OPTS	MAVEN_OPTS is set unconditionally in make_distribution.sh. Even with the 2GB set I still ran out of memory. PR follows shortly.
Script /dev/run-tests fails when IBM Java is used	When execute ./dev/run-tests with IBM Java, an exception occurs.This is due to difference of a "java version" format.
streaming driver with checkpointing unable to finalize leading to OOM	Spark streaming application when configured with checkpointing is filling driver's heap with multiple ZipFileInputStream instances as results of spark-assembly.jar (potentially some others like for example snappy-java.jar) getting repetitively referenced (loaded?). Java Finalizer can't finalize these ZipFileInputStream instances and it eventually takes all heap leading the driver to OOM crash.
No default RDD name for ones created by sc.textFile	Having a default name for an RDD created from a file is very handy.The feature was first added at commit: 7b877b2 but was later removed (probably by mistake) at commit: fc8b581.This change sets the default path of RDDs created via sc.textFile(...) to the path argument.
"ifelse","when","otherwise" unable to take Column as value	When passing a Column to ifelse, when, otherwise, it will error out with:attempt to replicate an object of type 'environment' The problems lies in the use of base R ifelse function, which is vectorized version of if ... else ... idiom, but it is unable to replicate a Column's job id as it is an environment. Considering callJMethod was never designed to be vectorized, the safe option is to replace ifelse with if ... else ... instead. However technically this is inconsistent to base R's ifelse, which is meant to be vectorized.
Writing to partitioned parquet table can fail with OOM	It is possible to have jobs fail with OOM when writing to a partitioned parquet table. While this was probably always possible, it is more likely in 1.6 due to the memory manager changes. The unified memory manager enables Spark to use more of the process memory (in particular, for execution) which gets us in this state more often. This issue can happen for libraries that consume a lot of memory, such as parquet. Prior to 1.6, these libraries would more likely use memory that spark was not using (i.e. from the storage pool). In 1.6, this storage memory can now be used for execution.
AnalysisException when multiple functions applied in GROUP BY clause	"I have following issue when trying to use functions in group by clause.Example:sqlCtx = HiveContext(sc) rdd = sc.parallelize([{'test_date': 1451400761}]) df = sqlCtx.createDataFrame(rdd) df.registerTempTable(""df"") Now, where I'm using single function it's OK. sqlCtx.sql(""select cast(test_date as timestamp) from df group by cast(test_date as timestamp)"").collect()[Row(test_date=datetime.datetime(2015, 12, 29, 15, 52, 41))] Where I'm using more than one function I'm getting AnalysisException sqlCtx.sql(""select date(cast(test_date as timestamp)) from df group by date(cast(test_date as timestamp))"").collect() Py4JJavaError: An error occurred while calling o38.sql.:org.apache.spark.sql.AnalysisException: expression 'test_date' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get."
User-specified JDBC driver should always take precedence	Spark SQL's JDBC data source allows users to specify an explicit JDBC driver to load using the driver argument, but in the current code it's possible that the user-specified driver will not be used when it comes time to actually create a JDBC connection.In a nutshell, the problem is that you might have multiple JDBC drivers on your classpath that claim to be able to handle the same subprotocol and there doesn't seem to be an intuitive way to control which of those drivers takes precedence.
result row size is wrong in UnsafeRowParquetRecordReader	When we write rows in UnsafeRowParquetRecordReader, we call `row.pointTo` at first, which assigns a wrong row size. We should reset the row size after writing all columns.
Bug in setMinPartitions function of StreamFileInputFormat	The maxSplitSize should be computed as:val maxSplitSize = Math.ceil(totalLen * 1.0 / minPartitions).toLong
When schema is specified, we should give better error message if actual row length doesn't match	The following code snippet reproduces this issue:from pyspark.sql.types import StructType, StructField, IntegerType, StringTypefrom pyspark.sql.types import Row schema = StructType([StructField("a", IntegerType()), StructField("b", StringType())]) rdd = sc.parallelize(range(10)).map(lambda x: Row(a=x)) df = sqlContext.createDataFrame(rdd, schema) df.show() An unintuitive ArrayIndexOutOfBoundsException exception is thrown in this case:Caused by: java.lang.ArrayIndexOutOfBoundsException: 1 at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:227) at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getAs(rows.scala:35) at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.isNullAt(rows.scala:36
SparkR: DataFrame's saveAsTable method has issues with the signature and HiveContext	There are several issues with the DataFrame's saveAsTable method in SparkR. Here is a summary of some of them. Hope this will help to fix the issues. 1.According to SparkR's saveAsTable(...) documentation, we can call the "saveAsTable(df, "myfile")" in order to store the dataframe.However, this signature isn't working. It seems that "source" and "mode" are forced according to signature.2. Within the method saveAsTable(...) it tries to retrieve the SQL context and tries to create/initialize source as parquet, but this is also failing because the context has to be Hive Context. Based on the error messages I see.3.In general the method fails when I try to call it with sqlContext 4.Also, it seems that SQL DataFrame.saveAsTable is deprecated, we could use df.write.saveAsTable(...) instead
Parameter explaination not very accurate for rdd function "aggregate"	Currently, RDD function aggregate's parameter doesn't explain well, especially parameter "zeroValue".It's necessary to let junior scala user know that "zeroValue" attend both "seqOp" and "combOp" phase.
sc.wholeTextFiles with spark.hadoop.cloneConf=true fails on secure Hadoop	On a secure hadoop cluster using pyspark or spark-shell in yarn client mode with spark.hadoop.cloneConf=true, start it up and wait for over 1 minute. Then try to use: val files = sc.wholeTextFiles("dir") files.collect() and it fails
Add a local sort operator to DataFrame used by randomSplit	"With ./bin/spark-shell --master=local-cluster[2,1,2014], the following code will provide overlapped rows for two DFs returned by the randomSplit.sqlContext.sql(""drop table if exists test"") val x = sc.parallelize(1 to 210) case class R(ID : Int) sqlContext.createDataFrame(x.map {R(_)}).write.format(""json"").saveAsTable(""bugsc1597"")var df = sql(""select distinct ID from test"")var Array(a, b) = df.randomSplit(Array(0.333, 0.667), 1234L) a.registerTempTable(""a"") b.registerTempTable(""b"") val intersectDF = a.intersect(b) intersectDF.show The reason is that {{sql(""select distinct ID from test"")} does not guarantee the ordering rows in a partition. It will be good to add a local sort operator to make row ordering within a partition deterministic."
Prepending base URI of job description is missing	The base URI of job description is not prepending in the current code, which makes access to this URI from YARN proxy return 404. The screenshot is also attached.
MapPartitionsRDD should clear reference to prev RDD	When `MapPartitionsRDD.clearDependencies()` is called, `MapPartitionsRDD` keep a reference to `prev` which may cause a memory leak.
Hive will fail if the schema of a parquet table has a very wide schema	To reproduce it, you can create a table with many many columns. You need to make sure that all of data type strings combined exceeds 4000 chars (strings are generated by HiveMetastoreTypes.toMetastoreType). Then, save the table as parquet. Because we will try to use a hive compatible way to store the metadata, we will set the serde to parquet serde. Then, when you load the table, you will see a java.lang.IllegalArgumentException thrown from Hive's TypeInfoUtils. I believe the cause is the same as SPARK-6024. Hive's parquet does not handle wide schema well and the data type string is truncated. Once you hit this problem, you will not be able to drop the table because Hive fails to evaluate drop table command. To at least provide a better workaround. We should see if we should have a native drop table call to metastore and if we should add a flag to disable saving a data source table's metadata in hive compatible way.
word2vec trainWordsCount gets overflow	the log of word2vec reports trainWordsCount = -785727483 during computation over a large dataset. I'll also add vocabsize to the log. Update the priority as it will affects the computation process.alpha =learningRate * (1 - numPartitions * wordCount.toDouble / (trainWordsCount + 1))
Sorting task error in Stages Page when yarn mode	
ML StopWordsRemover does not protect itself from column name duplication	"At work we were 'taking a closer look' at ML transformers&estimators and I spotted that anomally.On first look, resolution looks simple:Add to StopWordsRemover.transformSchema line (as is done in e.g.PCA.transformSchema,StandardScaler.transformSchema, OneHotEncoder.transformSchema):require(!schema.fieldNames.contains($(outputCol)), s""Output column ${$(outputCol)} already exists."")"
Details of batch in Streaming tab uses two Duration columns	"Details of batch" screen in Streaming tab in web UI uses two Duration columns. I think one should be "Processing Time" while the other "Job Duration".
Spark may attempt to rebuild application UI before finishing writing the event logs in possible race condition	As reported in SPARK-6950, it appears that sometimes the standalone master attempts to build an application's historical UI before closing the app's event log. This is still an issue for us in 1.5.2+, and I believe I've found the underlying cause.When stopping a SparkContext, the stop method stops the DAG scheduler,and then stops the event logger:Though it is difficult to follow the chain of events, one of the sequelae of stopping the DAG scheduler is that the master's rebuildSparkUI method is called. This method looks for the application's event logs, and its behavior varies based on the existence of an .inprogress file suffix. In particular, a warning is logged if this suffix exists:After calling the stop method on the DAG scheduler, the SparkContext stops the event logger:This renames the event log, dropping the .inprogress file sequence.As such, a race condition exists where the master may attempt to process the application log file before finalizing it.
Spark UI IndexOutOfBoundsException with dynamic allocation	Trying to load the web UI Executors page when using dynamic allocation running on yarn can lead to an IndexOutOfBoundsException Exception.I'm assuming the number of executors is changing as its trying to be loaded which is causing this as during this time it was letting executors go.HTTP ERROR 500Problem accessing /executors/. Reason:Server ErrorCaused by:java.lang.IndexOutOfBoundsException: 1058at scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:52)at scala.collection.immutable.Stream.apply(Stream.scala:185)at org.apache.spark.ui.exec.ExecutorsPage$.getExecInfo(ExecutorsPage.scala:180)at org.apache.spark.ui.exec.ExecutorsPage$$anonfun$11.apply(ExecutorsPage.scala:60)at org.apache.spark.ui.exec.ExecutorsPage$$anonfun$11.apply(ExecutorsPage.scala:59)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)at scala.collection.immutable.Range.foreach(Range.scala:141)at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)at scala.collection.AbstractTraversable.map(Traversable.scala:105)at org.apache.spark.ui.exec.ExecutorsPage.render(ExecutorsPage.scala:59)at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:79)at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:79)at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:69)at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)at org.spark-project.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:164)at org.spark-project.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)at org.spark-project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)at org.spark-project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)at org.spark-project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)at org.spark-project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)at org.spark-project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)at org.spark-project.jetty.server.handler.GzipHandler.handle(GzipHandler.java:264)at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)at org.spark-project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)at org.spark-project.jetty.server.Server.handle(Server.java:370)at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)at org.spark-project.jetty.http.HttpParser.parseNext(HttpParser.java:644)at org.spark-project.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)at org.spark-project.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)at java.lang.Thread.run(Thread.java:745)Simply reloading eventually gets the ui to come up so its not a blocker but not a very friendly experience either.
Names of input streams with receivers don't fit in Streaming page	Since the column for the names of input streams with receivers (under Input Rate) is fixed, the not-so-long names don't fit in Streaming page. 
ML StringIndexer does not protect itself from column name duplication	StringIndexerModel, when performing transform() does not check the schema of the input DataFrame. Because of that, it is possible to create a DataFrame containing columns with duplicated names.This issue is similar to SPARK-12711. StringIndexer could make use of transformSchema to assure that the input DataFrame schema is correct in sense of the parameters' values.
Work around memory leak in Snappy library	it looks like there is a memory leak in the snappy-java Java which we should work around in Spark. We could wait for a new version of the library, but I think we should just make a one- or two-line change in Spark to mask the issue.
Bad interaction between StarExpansion and ExtractWindowExpressions	"In the rule ExtractWindowExpressions, we simply replace alias by the corresponding attribute. However, this will cause an issue exposed by the following case:val data = Seq((""a"", ""b"", ""c"", 3), (""c"", ""b"", ""a"", 3)).toDF(""A"", ""B"", ""C"", ""num"").withColumn(""Data"", struct(""A"", ""B"",""C"")).drop(""A"").drop(""B"").drop(""C"")val winSpec =Window.partitionBy(""Data.A"", ""Data.B"").orderBy($""num"".desc)data.select($""*"", max(""num"").over(winSpec) as ""max"").explain(true)In this case, both Data.A and Data.B are alias in WindowSpecDefinition. If we replace these alias expression by their alias names, we are unable to know what they are since they will not be put in missingExpr too."
Pyspark Params.hasParam should not throw an error	Pyspark Params class has a method hasParam(paramName) which returns True if the class has a parameter by that name, but throws an AttributeError otherwise. There is not currently a way of getting a Boolean to indicate if a class has a parameter. With Spark 2.0 we could modify the existing behavior of hasParam or add an additional method with this functionality.In Python:from pyspark.ml.classification import NaiveBayes nb = NaiveBayes(smoothing=0.5) print nb.hasParam("smoothing")print nb.hasParam("notAParam") produces:True AttributeError: 'NaiveBayes' object has no attribute 'notAParam' However, in Scala: import org.apache.spark.ml.classification.NaiveBayes val nb  = new NaiveBayes() nb.hasParam("smoothing")nb.hasParam("notAParam") produces:true false
Map column would throw NPE if value is null	Create a map like{ "a": "somestring", "b": null}Query like SELECT col["b"] FROM t1; NPE would be thrown.
Race condition in MemoryStore.unrollSafely() causes memory leak	The unrollSafely() method in MemoryStore will progressively unroll the contents of a block iterator into memory. It works by reserving an initial chunk of unroll memory and periodically checking if more memory must be reserved as it unrolls the iterator. The memory reserved for performing the unroll is considered "pending" memory and is tracked on a per-task attempt ID bases in a map object named pendingUnrollMemoryMap. When the unrolled block is committed to storage memory in the tryToPut() method, a method named releasePendingUnrollMemoryForThisTask() is invoked and this pending memory is released. tryToPut() then proceeds to allocate the storage memory required for the block.The unrollSafely() method computes the amount of pending memory used for the unroll operation by saving the amount of unroll memory reserved for the particular task attempt ID at the start of the method in a variable named previousMemoryReserved and subtracting this value from the unroll memory dedicated to the task at the end of the method. This value is stored as the variable amountToTransferToPending. This amount is then subtracted from the per-task unrollMemoryMap and added to pendingUnrollMemoryMap.The amount of unroll memory consumed for the task is obtained from unrollMemoryMap via the currentUnrollMemoryForThisTask method. In order for the semantics of unrollSafely() to work, the value of unrollMemoryMap for the task returned by currentTaskAttemptId() must not be mutated between the computation of previousMemoryReserved and amountToTransferToPending. However, since there is no synchronization in place to ensure that computing both variables and updating the memory maps happens atomically, a race condition can occur when multiple threads for which currentTaskAttemptId() returns the same value are both trying to store blocks. This can lead to a negative value being computed for amountToTransferToPending, corrupting the unrollMemoryMap and pendingUnrollMemoryMap memory maps which in turn can lead to the memory manager leaking unroll memory
Problem accessing Web UI /logPage/ on Microsoft Windows	A calculated path can be built with a mixture of \ and /. The problem is with the "\" character. This is there because, when using Windows, the "path.separator" System Property used by the java.io.File class is set to "\". This is what is used to create the initial part of the path. The "/" in the rest of the path are hard-coded in the Spark code.Problem accessing /logPage/. Reason:Server ErrorCaused by:java.net.URISyntaxException: Illegal character in path at index 1: .\work/app-20160129163017-0039/0/at java.net.URI$Parser.fail(Unknown Source)at java.net.URI$Parser.checkChars(Unknown Source)at java.net.URI$Parser.parseHierarchical(Unknown Source)at java.net.URI$Parser.parse(Unknown Source)at java.net.URI.<init>(Unknown Source)at org.apache.spark.deploy.worker.ui.LogPage.getLog(LogPage.scala:141)at org.apache.spark.deploy.worker.ui.LogPage.render(LogPage.scala:78)at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:79)at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:79)at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:69)at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)at org.spark-project.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)at org.spark-project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)at org.spark-project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)at org.spark-project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)at org.spark-project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)at org.spark-project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)at org.spark-project.jetty.server.handler.GzipHandler.handle(GzipHandler.java:264)at org.spark-project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)at org.spark-project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)at org.spark-project.jetty.server.Server.handle(Server.java:370)at org.spark-project.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)at org.spark-project.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)at org.spark-project.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)at org.spark-project.jetty.http.HttpParser.parseNext(HttpParser.java:644)at org.spark-project.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)at org.spark-project.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)at org.spark-project.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)at org.spark-project.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)at java.lang.Thread.run(Unknown Source)
PySpark ML persistence failed when handle no default value parameter	This defect find when implement task spark-13033. When add below code to doctest.It looks like _transfer_params_from_java did not consider the params which do not have default value and we should handle them.import os, tempfile path = tempfile.mkdtemp() aftsr_path = path + "/aftsr" aftsr.save(aftsr_path) aftsr2 = AFTSurvivalRegression.load(aftsr_path) >>> import os, tempfile>>> path = tempfile.mkdtemp()>>> aftsr_path = path + "/aftsr">>> aftsr.save(aftsr_path)>>> aftsr2 = AFTSurvivalRegression.load(aftsr_path)Exception detail.ir2 = IsotonicRegression.load(ir_path)Exception raised:Traceback (most recent call last):File "C:\Python27\lib\doctest.py", line 1289, in runcompileflags, 1) in test.globsFile "<doctest __main.IsotonicRegression[11]>", line 1, inir2 = IsotonicRegression.load(ir_path)File "C:\aWorkFolder\spark\spark-1.6.0-bin-hadoop2.6\spark-1.6.0-bin-hadoop2.6\python\lib\pyspark.zip\pyspark\ml\util.py", line 194, in loadreturn cls.read().load(path)File "C:\aWorkFolder\spark\spark-1.6.0-bin-hadoop2.6\spark-1.6.0-bin-hadoop2.6\python\lib\pyspark.zip\pyspark\ml\util.py", line 148, in loadinstance.transfer_params_from_java()File "C:\aWorkFolder\spark\spark-1.6.0-bin-hadoop2.6\spark-1.6.0-bin-hadoop2.6\python\lib\pyspark.zip\pyspark\ml\wrapper.py", line 82, in tranfer_params_from_javavalue = _java2py(sc, self._java_obj.getOrDefault(java_param))File "C:\aWorkFolder\spark\spark-1.6.0-bin-hadoop2.6\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip\py4j\java_gateway.py", line 813, in_callanswer, self.gateway_client, self.target_id, self.name)File "C:\aWorkFolder\spark\spark-1.6.0-bin-hadoop2.6\spark-1.6.0-bin-hadoop2.6\python\lib\pyspark.zip\pyspark\sql\utils.py", line 45, in decoreturn f(a, *kw)File "C:\aWorkFolder\spark\spark-1.6.0-bin-hadoop2.6\spark-1.6.0-bin-hadoop2.6\python\lib\py4j-0.9-src.zip\py4j\protocol.py", line 308, in get_eturn_valueformat(target_id, ".", name), value)Py4JJavaError: An error occurred while calling o351.getOrDefault.: java.util.NoSuchElementException: Failed to find a default value for weightColat org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:647)at org.apache.spark.ml.param.Params$$anonfun$getOrDefault$2.apply(params.scala:647)at scala.Option.getOrElse(Option.scala:120)at org.apache.spark.ml.param.Params$class.getOrDefault(params.scala:646)at org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:43)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.lang.reflect.Method.invoke(Method.java:483)at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)at py4j.Gateway.invoke(Gateway.java:259)at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)at py4j.commands.CallCommand.execute(CallCommand.java:79)at py4j.GatewayConnection.run(GatewayConnection.java:209)at java.lang.Thread.run(Thread.java:745)
DAG visualization does not render correctly for jobs	Whenever I try to open the DAG for a job, I get something like this:Obviously the svg doesn't get resized, but if I resize it manually, only the first of four stages in the DAG is shown.The js console says (variable v is null in peg$c34):
ML Model Selection via Train Validation Split example uses incorrect data	The Model Selection via Train Validation Split example uses classification data for a regression problem, and so returns the appropriate errors when run.
Replace GraphImpl.fromExistingRDDs by Graph	`GraphImpl.fromExistingRDDs` expects preprocessed vertex RDD as input. We call it in LDA without validating this requirement. So it might introduce errors. Replacing it by `Gpaph.apply` would be safer and more proper because it is a public API.
TaskSetManager.dequeueSpeculativeTask compares Option[String] and String directly.	"TaskSetManager.dequeueSpeculativeTask compares Option[String] and String directly. if (TaskLocality.isAllowed(locality, TaskLocality.RACK_LOCAL)){ for (rack <- sched.getRackForHost(host)) { for (index <- speculatableTasks if canRunOnHost(index)) { val racks = tasks(index).preferredLocations.map(_.host).map(sched.getRackForHost)// racks: Seq[Option[String]] and rack: String if (racks.contains(rack)) { speculatableTasks -= index return Some((index, TaskLocality.RACK_LOCAL))}}}}"
NullPointerException when either HADOOP_CONF_DIR or YARN_CONF_DIR is not readable	NPE is throw from the yarn Client.scala because File.listFiles() can return null on directory that it doesn't have permission to list. This is the code fragment in question:// In org/apache/spark/deploy/yarn/Client.scala    Seq("HADOOP_CONF_DIR", "YARN_CONF_DIR").foreach { envKey =>      sys.env.get(envKey).foreach { path =>        val dir = new File(path)        if (dir.isDirectory()) {          // dir.listFiles() can return null          dir.listFiles().foreach { file =>            if (file.isFile && !hadoopConfFiles.contains(file.getName())) {              hadoopConfFiles(file.getName()) = file            }          }        }      }    }To reproduce, simply do:sudo mkdir /tmp/confsudo chmod 700 /tmp/confexport HADOOP_CONF_DIR=/etc/hadoop/confexport YARN_CONF_DIR=/tmp/confspark-submit --master yarn-client SimpleApp.pyIt fails on any Spark app. Though not important, the SimpleApp.py I used looks like this:from pyspark import SparkContextsc = SparkContext(None, "Simple App")data = [1, 2, 3, 4, 5]distData = sc.parallelize(data)total = distData.reduce(lambda a, b: a + b)print("Total: %i" % total)
Cannot drop table whose name starts with underscore	"Spark shell snippet for reproduction:sqlContext.sql(""CREATE TABLE `_a`(i INT)"") // This one works.sqlContext.sql(""DROP TABLE `_a`"") // This one failed. Basically, we cannot drop a table starting with _ in Spark 1.6.0. Master is fine.The reason is that, after being parsed, DROP TABLE is firstly handled by DropTable. Then, DropTable constructs a new DROP TABLE SQL statement and delegate it to Hive. However, the table name in the constructed SQL statement isn't quoted while Hive lexer doesn't allow unquoted identifiers to start with underscore."
Predicate can't be pushed through project with nondeterministic field	"The following Spark shell snippet reproduces this issue:import org.apache.spark.sql.functions._val parallelism = 8 // Adjust this to default parallelismval df = sqlContext.  range(2 * parallelism). // 8 partitions, 2 elements per partition  select(    col(""id""),    monotonicallyIncreasingId().as(""long_id"")  )df.show()monotonicallyIncreasingId is nondeterministic"
`spark.storage.memoryMapThreshold` has two kind of the value.	"`spark.storage.memoryMapThreshold` has two kind of the value, one is 2*1024*1024 as integer and the other one is '2m' as string.""2m"" is recommanded in document but it will go wrong if the code goes into TransportConf#memoryMapBytes.Useage of the `spark.storage.memoryMapThreshold`:"
Do not submit stage until its dependencies map outputs are registered	We should track pending tasks by partition ID instead of Task objects.  Before this, failure & retry could result in a case where a stage got submitted before the map output from its dependencies get registered. This was due to an error in the condition for registering map outputs.  More complete explanation of the original problem:  1. while shuffle stage was retry, there may have 2 taskSet running.  we call the 2 taskSet:taskSet0.0, taskSet0.1, and we know, taskSet0.1 will re-run taskSet0.0's un-complete task  if taskSet0.0 was run all the task that the taskSet0.1 not complete yet but covered the partitions.  then stage is Available is true.    def isAvailable: Boolean = {      if (!isShuffleMap) {        true      } else {        numAvailableOutputs == numPartitions      }    }   but stage.pending task is not empty, to protect register mapStatus in mapOutputTracker.  because if task is complete success, pendingTasks is minus Task in reference-level because the task is not override hashcode() and equals()  pendingTask -= task  but numAvailableOutputs is according to partitionID.  here is the testcase to prove:    test("Make sure mapStage.pendingtasks is set() " +      "while MapStage.isAvailable is true while stage was retry ") {      val firstRDD = new MyRDD(sc, 6, Nil)      val firstShuffleDep = new ShuffleDependency(firstRDD, null)      val firstShuyffleId = firstShuffleDep.shuffleId      val shuffleMapRdd = new MyRDD(sc, 6, List(firstShuffleDep))      val shuffleDep = new ShuffleDependency(shuffleMapRdd, null)      val shuffleId = shuffleDep.shuffleId      val reduceRdd = new MyRDD(sc, 2, List(shuffleDep))      submit(reduceRdd, Array(0, 1))      complete(taskSets(0), Seq(        (Success, makeMapStatus("hostB", 1)),        (Success, makeMapStatus("hostB", 2)),        (Success, makeMapStatus("hostC", 3)),        (Success, makeMapStatus("hostB", 4)),        (Success, makeMapStatus("hostB", 5)),        (Success, makeMapStatus("hostC", 6))      ))      complete(taskSets(1), Seq(        (Success, makeMapStatus("hostA", 1)),        (Success, makeMapStatus("hostB", 2)),        (Success, makeMapStatus("hostA", 1)),        (Success, makeMapStatus("hostB", 2)),        (Success, makeMapStatus("hostA", 1))      ))      runEvent(ExecutorLost("exec-hostA"))      runEvent(CompletionEvent(taskSets(1).tasks(0), Resubmitted, null, null, null, null))      runEvent(CompletionEvent(taskSets(1).tasks(2), Resubmitted, null, null, null, null))      runEvent(CompletionEvent(taskSets(1).tasks(0),        FetchFailed(null, firstShuyffleId, -1, 0, "Fetch Mata data failed"),        null, null, null, null))      scheduler.resubmitFailedStages()      runEvent(CompletionEvent(taskSets(1).tasks(0), Success,        makeMapStatus("hostC", 1), null, null, null))      runEvent(CompletionEvent(taskSets(1).tasks(2), Success,        makeMapStatus("hostC", 1), null, null, null))      runEvent(CompletionEvent(taskSets(1).tasks(4), Success,        makeMapStatus("hostC", 1), null, null, null))      runEvent(CompletionEvent(taskSets(1).tasks(5), Success,        makeMapStatus("hostB", 2), null, null, null))      val stage = scheduler.stageIdToStage(taskSets(1).stageId)      assert(stage.attemptId == 2)      assert(stage.isAvailable)      assert(stage.pendingTasks.size == 0)    }
Spark should not retry a stage infinitely on a FetchFailedException	While investigating SPARK-5928, I noticed some very strange behavior in the way spark retries stages after a FetchFailedException. It seems that on a FetchFailedException, instead of simply killing the task and retrying, Spark aborts the stage and retries. If it just retried the task, the task might fail 4 times and then trigger the usual job killing mechanism. But by killing the stage instead, the max retry logic is skipped (it looks to me like there is no limit for retries on a stage).  After a bit of discussion with Kay Ousterhout, it seems the idea is that if a fetch fails, we assume that the block manager we are fetching from has failed, and that it will succeed if we retry the stage w/out that block manager. In that case, it wouldn't make any sense to retry the task, since its doomed to fail every time, so we might as well kill the whole stage. But this raises two questions:  1) Is it really safe to assume that a FetchFailedException means that the BlockManager has failed, and ti will work if we just try another one? SPARK-5928 shows that there are at least some cases where that assumption is wrong. Even if we fix that case, this logic seems brittle to the next case we find. I guess the idea is that this behavior is what gives us the "R" in RDD ... but it seems like its not really that robust and maybe should be reconsidered.  2) Should stages only be retried a limited number of times? It would be pretty easy to put in a limited number of retries per stage. Though again, we encounter issues with keeping things resilient. Theoretically one stage could have many retries, but due to failures in different stages further downstream, so we might need to track the cause of each retry as well to still have the desired behavior.  In general it just seems there is some flakiness in the retry logic. This is the only reproducible example I have at the moment, but I vaguely recall hitting other cases of strange behavior w/ retries when trying to run long pipelines. Eg., if one executor is stuck in a GC during a fetch, the fetch fails, but the executor eventually comes back and the stage gets retried again, but the same GC issues happen the second time around, etc.  Copied from SPARK-5928, here's the example program that can regularly produce a loop of stage failures. Note that it will only fail from a remote fetch, so it can't be run locally ¨C I ran with MASTER=yarn-client spark-shell --num-executors 2 --executor-memory 4000m      val rdd = sc.parallelize(1 to 1e6.toInt, 1).map{ ignore =>        val n = 3e3.toInt        val arr = new Array[Byte](n)        //need to make sure the array doesn't compress to something small        scala.util.Random.nextBytes(arr)        arr      }      rdd.map { x => (1, x)}.groupByKey().count()
Executor table on Stage page should sort by Executor ID numerically, not lexically	Page loads with a table like this: After clicking "Executor ID" to sort by that column, it sorts numerically:
SQL operator and condition precedence is not honoured	The following query from the SQL Logic Test suite fails to parse:  SELECT DISTINCT * FROM t1 AS cor0 WHERE NOT ( - _2 + - 39 ) IS NULL  while the following (equivalent) does parse correctly:  SELECT DISTINCT * FROM t1 AS cor0 WHERE NOT (( - _2 + - 39 ) IS NULL)  SQLite, MySQL and Oracle (and probably most SQL implementations) define IS with higher precedence than NOT, so the first query is valid and well-defined.
Sometimes the status of finished job show on JobHistory UI will be active, and never update.	When I run a SparkPi job, the status of the job on JobHistory UI was 'active'. After the job finished for a long time, the status on JobHistory UI never update again, and the job keep in the 'Incomplete applications' list.   This problem appears occasionally. And the configuration of JobHistory is default value.
Spark build should not use lib_managed for dependencies	unnecessary duplication (I will have those libraries under ./m2, via maven anyway)  every time I call make-distribution I lose lib_managed (via mvn clean install) and have to wait to download again all jars next time I use sbt  Eclipse does not handle relative paths very well (source attachments from lib_managed don¡¯t always work)  it's not the default configuration. If we stray from defaults I think there should be a clear advantage.  Digging through history, the only reference to `retrieveManaged := true` I found was in f686e3d, from July 2011 ("Initial work on converting build to SBT 0.10.1"). My guess this is purely an accident of porting the build form Sbt 0.7.x and trying to keep the old project layout.  If there are reasons for keeping it, please comment (I didn't get any answers on the dev mailing list)
SparkR createDataFrame is slow	For example calling `createDataFrame` on the data from http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv takes a really long time  This is mainly because we try to convert a DataFrame to a List in order to parallelize it by rows and the conversion from DF to list is very slow for large data frames.
Ensure ContextCleaner actually triggers clean ups	Right now it cleans up old references only through natural GCs, which may not occur if the driver has infinite RAM. We should do a periodic GC to make sure that we actually do clean things up. Something like once per 30 minutes seems relatively inexpensive.
Poor Python UDF performance because of RDD caching	We have been running into performance problems using Python UDFs with DataFrames at large scale.  From the implementation of BatchPythonEvaluation, it looks like the goal was to reuse the PythonRDD code. It caches the entire child RDD so that it can do two passes over the data. One to give to the PythonRDD, then one to join the python lambda results with the original row (which may have java objects that should be passed through).  In addition, it caches all the columns, even the ones that don't need to be processed by the Python UDF. In the cases I was working with, I had a 500 column table, and i wanted to use a python UDF for one column, and it ended up caching all 500 columns.  http://apache-spark-developers-list.1001551.n3.nabble.com/Python-UDF-performance-at-large-scale-td12843.html
Analysis exception when using "NULL IN (...)": invalid cast	The following query throws an analysis exception:  SELECT * FROM t WHERE NULL NOT IN (1, 2, 3);  The exception is:  org.apache.spark.sql.AnalysisException: invalid cast from int to null;   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:38)   at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:42)   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:66)   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:52)  Here is a test that can be added to AnalysisSuite to check the issue:    test("SPARK-XXXX regression test") {      val plan = Project(Alias(In(Literal(null), Seq(Literal(1), Literal(2))), "a")() :: Nil,        LocalRelation()      )      caseInsensitiveAnalyze(plan)    }  Note that this kind of query is a corner case, but it is still valid SQL. An expression such as "NULL IN (...)" or "NULL NOT IN (...)" always gives NULL as a result, even if the list contains NULL. So it is safe to translate these expressions to Literal(null) during analysis.
AttributeReference equals method only compare name, exprId and dataType	The AttributeReference "equals" method only accept as different objects with different name, expression id or dataType. With this behavior when I tried to do a "transformExpressionsDown" and try to transform qualifiers inside "AttributeReferences", these objects are not replaced, because the transformer considers them equal.  I propose to add to the "equals" method this variables:  name, dataType, nullable, metadata, epxrId, qualifiers
RDD#toDebugString fails if any cached RDD has invalid partitions	Repro:  sc.textFile("/ThisFileDoesNotExist").cache()  sc.parallelize(0 until 100).toDebugString  Output:  java.io.IOException: Not a file: /ThisFileDoesNotExist   at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:215)   at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)   at scala.Option.getOrElse(Option.scala:120)   at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)   at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)   at scala.Option.getOrElse(Option.scala:120)   at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)   at org.apache.spark.storage.RDDInfo$.fromRdd(RDDInfo.scala:59)   at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:1455)   at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:1455)   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)   at scala.collection.Iterator$class.foreach(Iterator.scala:727)   at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)   at scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:206)   at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)   at scala.collection.AbstractTraversable.map(Traversable.scala:105)   at org.apache.spark.SparkContext.getRDDStorageInfo(SparkContext.scala:1455)   at org.apache.spark.rdd.RDD.debugSelf$1(RDD.scala:1573)   at org.apache.spark.rdd.RDD.firstDebugString$1(RDD.scala:1607)   at org.apache.spark.rdd.RDD.toDebugString(RDD.scala:1637  This is because toDebugString gets all the partitions from all RDDs, which fails (via SparkContext#getRDDStorageInfo). This pathway should definitely be resilient to other RDDs being invalid (and getRDDStorageInfo should probably also be).
Deser primitive class with Java serialization	Objects that contain as property a primitive Class, can not be deserialized using java serde. Class.forName does not work for primitives.  Exemple of object:  class Foo extends Serializable  { val intClass = classOf[Int] }
showDagViz will cause java.lang.OutOfMemoryError: Java heap space	HTTP ERROR 500  Problem accessing /history/app-20150708101140-0018/jobs/job/. Reason:  Server Error  Caused by:  java.lang.OutOfMemoryError: Java heap space  at java.util.Arrays.copyOf(Arrays.java:2367)  at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:130)  at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:114)  at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:415)  at java.lang.StringBuilder.append(StringBuilder.java:132)  at scala.collection.mutable.StringBuilder.append(StringBuilder.scala:207)  at org.apache.spark.ui.scope.RDDOperationGraph$$anonfun$org$apache$spark$ui$scope$RDDOperationGraph$$makeDotSubgraph$2.apply(RDDOperationGraph.scala:192)  at org.apache.spark.ui.scope.RDDOperationGraph$$anonfun$org$apache$spark$ui$scope$RDDOperationGraph$$makeDotSubgraph$2.apply(RDDOperationGraph.scala:191)  at scala.collection.immutable.Stream.foreach(Stream.scala:547)  at org.apache.spark.ui.scope.RDDOperationGraph$.org$apache$spark$ui$scope$RDDOperationGraph$$makeDotSubgraph(RDDOperationGraph.scala:191)  at org.apache.spark.ui.scope.RDDOperationGraph$.makeDotFile(RDDOperationGraph.scala:170)  at org.apache.spark.ui.UIUtils$$anonfun$showDagViz$1.apply(UIUtils.scala:361)  at org.apache.spark.ui.UIUtils$$anonfun$showDagViz$1.apply(UIUtils.scala:357)  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)  at scala.collection.immutable.List.foreach(List.scala:318)  at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)  at scala.collection.AbstractTraversable.map(Traversable.scala:105)  at org.apache.spark.ui.UIUtils$.showDagViz(UIUtils.scala:357)  at org.apache.spark.ui.UIUtils$.showDagVizForJob(UIUtils.scala:335)  at org.apache.spark.ui.jobs.JobPage.render(JobPage.scala:317)  at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:79)  at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:79)  at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:69)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)  at org.spark-project.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)  at org.spark-project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)  at org.spark-project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)  at org.spark-project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)  at org.spark-project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)  at org.spark-project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
support CJK characters in collect()	Spark gives an error message and does not show the output when a field of the result DataFrame contains characters in CJK.  I found out that SerDe in R API only supports ASCII format for strings right now as commented in source code.   So, I fixed SerDe.scala a little to support CJK as the file attached.   I did not care efficiency, but just wanted to see if it works.  people.json  {"name":"??"}  {"name":"???123", "age":30}  {"name":"Justin", "age":19}    df <- read.df(sqlContext, "./people.json", "json")  head(df)    Error in rawtochar(string) : embedded nul in string : '\0 \x98'  core/src/main/scala/org/apache/spark/api/r/SerDe.scala    // NOTE: Only works for ASCII right now    def writeString(out: DataOutputStream, value: String): Unit = {      val len = value.length      out.writeInt(len + 1) // For the \0      out.writeBytes(value)      out.writeByte(0)
ORC data source creates a schema with lowercase table names	Steps to reproduce:  sqlContext.range(0, 10).select('id as "Acol").write.format("orc").save("/tmp/foo")    sqlContext.read.format("orc").load("/tmp/foo").schema("Acol")  //java.lang.IllegalArgumentException: Field "Acol" does not exist.    sqlContext.read.format("orc").load("/tmp/foo").schema("acol")  //org.apache.spark.sql.types.StructField = StructField(acol,LongType,true)    sqlContext.read.format("orc").load("/tmp/foo").select("Acol").show()  //+----+  |Acol|  +----+  |   1|  |   5|  |   3|  |   4|  |   7|  |   2|  |   6|  |   8|  |   9|  |   0|  +----+
Dynamic allocation kills busy executors on race condition	By using the dynamic allocation, sometimes it occurs false killing for those busy executors. Some executors with assignments will be killed because of being idle for enough time (say 60 seconds). The root cause is that the Task-Launch listener event is asynchronized.  For example, some executors are under assigning tasks, but not sending out the listener notification yet. Meanwhile, the dynamic allocation's executor idle time is up (e.g., 60 seconds). It will trigger killExecutor event at the same time.  the timer expiration starts before the listener event arrives.  Then, the task is going to run on top of that killed/killing executor. It will lead to task failure finally.  Here is the proposal to fix it. We can add the force control for killExecutor. If the force control is not set (i.e., false), we'd better to check if the executor under killing is idle or busy. If the current executor has some assignment, we should not kill that executor and return back false (to indicate killing failure). In dynamic allocation, we'd better to turn off force killing (i.e., force = false), we will meet killing failure if tries to kill a busy executor. And then, the executor timer won't be invalid. Later on, the task assignment event arrives, we can remove the idle timer accordingly. So that we can avoid false killing for those busy executors in dynamic allocation.  For the rest of usages, the end users can decide if to use force killing or not by themselves. If to turn on that option, the killExecutor will do the action without any status checking.
HiveHBaseTableInputFormat can'be cached	Below exception occurs in Spark On HBase function.  java.lang.RuntimeException: java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture@11c6577 rejected from java.util.concurrent.ThreadPoolExecutor@3414350b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 17451]  When an executor has many cores, the tasks belongs to same RDD will using the same InputFormat. But the HiveHBaseTableInputFormat is not thread safety.  So I think we should add a config to enable cache InputFormat or not.
Avoid reloading Hadoop classes like UserGroupInformation	Some hadoop classes contains global information such as authentication in UserGroupInformation. If we load them again in `IsolatedClientLoader`, the message they carry will be dropped.  So we should treat hadoop classes as "shared" too.
Drivers run in cluster mode on mesos may not have spark-env variables available	This issue definitely affects Mesos mode, but may effect complex standalone topologies as well.  When running spark-submit with  --deploy-mode cluster  environment variables set in spark-env.sh that are not prefixed with SPARK_ are not available in the driver process. The behavior I expect is that any variables set in spark-env.sh are available on the driver and all executors.  spark-env.sh is executed by load-spark-env.sh which uses an environment variable SPARK_ENV_LOADED [code] to ensure that it is only run once. When using the RestSubmissionClient, spark submit propagates all environment variables that are prefixed with SPARK_ [code] to the MesosRestServer where they are used to initialize the driver [code]. During this process, SPARK_ENV_LOADED is propagated to the new driver process (since running spark submit has caused load-spark-env.sh to be run on the submitter's machine) [code]. Now when load-spark-env.sh is called by MesosClusterScheduler SPARK_ENV_LOADED is set and spark-env.sh is never sourced.  This gist shows the testing setup I used while investigating this issue. An example invocation looked like  spark-1.5.0-SNAPSHOT-bin-custom-spark/bin/spark-submit --deploy-mode cluster --master mesos://172.31.34.154:7077 --class Test spark-env-var-test_2.10-0.1-SNAPSHOT.jar
Spark should create local temporary directories in Mesos sandbox when launched with Mesos	Currently Spark creates temporary directories with Utils.getConfiguredLocalDirs, and it writes to YARN directories if YARN is detected, otherwise just writes in a temporary directory in the host.  However, Mesos does create a directory per task and ideally Spark should use that directory to create its local temporary directories since it then can be cleaned up when the task is gone and not left on the host or cleaned until reboot.
Avoid unnecessary redirects in the Spark Web UI	When opening Spark's Web UI for an application, there are a number of redirects which makes it feel slow. We can avoid one of them.  vagrant@localhost:~ $ curl -i http://localhost:18080/history/application_1438648766072_0005/  HTTP/1.1 302 Found  Location: http://localhost:18080/history/application_1438648766072_0005/  Content-Length: 0  Server: Jetty(8.y.z-SNAPSHOT)    vagrant@localhost:~ $ curl -i http://localhost:18080/history/application_1438648766072_0005/  HTTP/1.1 302 Found  Location: http://localhost:18080/history/application_1438648766072_0005/jobs  Content-Length: 0  Server: Jetty(8.y.z-SNAPSHOT)    vagrant@localhost:~ $ curl -i http://localhost:18080/history/application_1438648766072_0005/jobs  HTTP/1.1 302 Found  Location: http://localhost:18080/history/application_1438648766072_0005/jobs/  Content-Length: 0  Server: Jetty(8.y.z-SNAPSHOT)    vagrant@localhost:~ $ curl -i http://localhost:18080/history/application_1438648766072_0005/jobs/  HTTP/1.1 200 OK  Content-Type: text/html;charset=UTF-8  Cache-Control: no-cache, no-store, must-revalidate  Content-Length: 5267  Server: Jetty(8.y.z-SNAPSHOT)
Auto infer partition schema of HadoopFsRelation should should respected the user specified one	This code is copied from the hadoopFsRelationSuite.scala  partitionedTestDF = (for {      i <- 1 to 3      p2 <- Seq("foo", "bar")    } yield (i, s"val_$i", 1, p2)).toDF("a", "b", "p1", "p2")        withTempPath { file =>        val input = partitionedTestDF.select('a, 'b, 'p1.cast(StringType).as('ps), 'p2)          input          .write          .format(dataSourceName)          .mode(SaveMode.Overwrite)          .partitionBy("ps", "p2")          .saveAsTable("t")          input          .write          .format(dataSourceName)          .mode(SaveMode.Append)          .partitionBy("ps", "p2")          .saveAsTable("t")          val realData = input.collect()        withTempTable("t") {          checkAnswer(sqlContext.table("t"), realData ++ realData)        }      }    java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.spark.unsafe.types.UTF8String   at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getUTF8String(rows.scala:45)   at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getUTF8String(rows.scala:220)   at org.apache.spark.sql.catalyst.expressions.JoinedRow.getUTF8String(JoinedRow.scala:102)   at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:62)   at org.apache.spark.sql.execution.datasources.DataSourceStrategy$$anonfun$17$$anonfun$apply$9.apply(DataSourceStrategy.scala:212)   at org.apache.spark.sql.execution.datasources.DataSourceStrategy$$anonfun$17$$anonfun$apply$9.apply(DataSourceStrategy.scala:212)   at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)   at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)   at scala.collection.Iterator$class.foreach(Iterator.scala:727)   at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)   at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)   at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)   at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)   at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)   at scala.collection.AbstractIterator.to(Iterator.scala:1157)   at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)   at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)   at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)   at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)   at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:903)   at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:903)   at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1846)   at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1846)   at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)   at org.apache.spark.scheduler.Task.run(Task.scala:88)   at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   at java.lang.Thread.run(Thread.java:745)  07:44:01.344 ERROR org.apache.spark.executor.Executor: Exception in task 14.0 in stage 3.0 (TID 206)  java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.spark.unsafe.types.UTF8String   at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getUTF8String(rows.scala:45)   at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getUTF8String(rows.scala:220)   at org.apache.spark.sql.catalyst.expressions.JoinedRow.getUTF8String(JoinedRow.scala:102)   at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:62)   at org.apache.spark.sql.execution.datasources.DataSourceStrategy$$anonfun$17$$anonfun$apply$9.apply(DataSourceStrategy.scala:212)   at org.apache.spark.sql.execution.datasources.DataSourceStrategy$$anonfun$17$$anonfun$apply$9.apply(DataSourceStrategy.scala:212)   at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)   at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)   at scala.collection.Iterator$class.foreach(Iterator.scala:727)   at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)   at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)   at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)   at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)   at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)   at scala.collection.AbstractIterator.to(Iterator.scala:1157)   at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)   at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)   at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)   at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)   at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:903)   at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:903)   at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1846)   at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1846)   at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)   at org.apache.spark.scheduler.Task.run(Task.scala:88)   at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   at java.lang.Thread.run(Thread.java:745)
PySpark DenseVector, SparseVector should override __eq__ and __hash__	See SPARK-9750.  PySpark DenseVector and SparseVector do not override the equality operator properly. They should use semantics, not representation, for comparison. (This is what Scala currently does.)
ISO DateTime parser is too strict	The DateTime parser requires 3 millisecond digits, but that is not part of the official ISO8601 spec.  https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L132  https://en.wikipedia.org/wiki/ISO_8601  This results in the following exception when trying to parse datetime columns  java.text.ParseException: Unparseable date: "0001-01-01T00:00:00GMT-00:00"  Josh Rosen Reynold Xin
Validate usages of Runtime.getRuntime.addShutdownHook	While refactoring calls to Utils.addShutdownHook to spark ShutdownHookManager in PR #8109, I've seen instances of calls to Runtime.getRuntime.addShutdownHook:  org\apache\spark\deploy\ExternalShuffleService.scala:126  org\apache\spark\deploy\mesos\MesosClusterDispatcher.scala:113  org\apache\spark\storage\ExternalBlockStore.scala:181  Comment from @vanzin:  "From a quick look, it seems that at least the one in ExternalBlockStore should be changed; the other two seem to be separate processes (i.e. they are not part of a Spark application) so that's questionable. But I'd say leave it for a different change (maybe file a separate bug so it doesn't fall through the cracks)."
Remove old Yarn MR classpath api support for Spark Yarn client	Since now we only support Yarn stable API, so here propose to remove old MRConfig.DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH api support in 2.0.
Improve readability of DAGScheduler	There are many opportunities for improving DAGScheduler's readability. This issue will represent more of an incremental process than one specific patch.
Hive warehouse dir not set in current directory when not providing hive-site.xml	When running spark in local environment (for unit-testing purpose) and without providing any `hive-site.xml, databases apart from the default one are created in Hive default hive.metastore.warehouse.dir and not in the current directory (as stated in [Spark docs](http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables)). This code snippet, tested with Spark 1.3.1 demonstrates the issue: https://github.com/tmnd1991/spark-hive-bug/blob/master/src/main/scala/Main.scala You cane see that the exception is thrown when executing the CREATE DATABASE STATEMENT, stating that is `Unable to create database path file:/user/hive/warehouse/abc.db, failed to create database abc)` where is `/user/hive/warehouse/abc.db`is not the current directory as stated in the docs.
Validate i, j in apply (Dense and Sparse Matrices)	Given row_ind should be less than the number of rows  Given col_ind should be less than the number of cols.  The current code in master gives unpredictable behavior for such cases.
PySpark filters with datetimes mess up when datetimes have timezones	PySpark appears to ignore timezone information when filtering on (and working in general with) datetimes.  Please see the example below. The generated filter in the query plan is 5 hours off (my computer is EST).  In [1]: df = sc.sql.createDataFrame([], StructType([StructField("dt", TimestampType())]))    In [2]: df.filter(df.dt > datetime(2000, 01, 01, tzinfo=UTC)).explain()  Filter (dt#9 > 946702800000000)   Scan PhysicalRDD[dt#9]  Note that 946702800000000 == Sat 1 Jan 2000 05:00:00 UTC
HiveContext is not used with keytab principal but with user principal/unix username	`bin/spark-submit --num-executors 1 --executor-cores 5 --executor-memory 5G --driver-java-options -XX:MaxPermSize=4G --driver-class-path lib/datanucleus-api-jdo-3.2.6.jar:lib/datanucleus-core-3.2.10.jar:lib/datanucleus-rdbms-3.2.9.jar:conf/hive-site.xml --files conf/hive-site.xml --master yarn --principal sparkjob --keytab /etc/security/keytabs/sparkjob.keytab --conf spark.yarn.executor.memoryOverhead=18000 --conf "spark.executor.extraJavaOptions=-XX:MaxPermSize=4G" --conf spark.eventLog.enabled=false ~/test.py`  With:  #!/usr/bin/python  from pyspark import SparkContext  from pyspark.sql import HiveContext  sc = SparkContext()  sqlContext = HiveContext(sc)  query = """ SELECT * FROM fm.sk_cluster """  rdd = sqlContext.sql(query)  rdd.registerTempTable("test")  sqlContext.sql("CREATE TABLE wcs.test LOCATION '/tmp/test_gl' AS SELECT * FROM test")  Ends up with:  Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denie  d: user=ua80tl, access=READ_EXECUTE, inode="/tmp/test_gl/.hive-staging_hive_2015-08-24_10-43-09_157_78057390024057878  34-1/ext-10000":sparkjob:hdfs:drwxr-x--  (Our umask denies read access to other by default)
GeneralizedLinearModel doesn't unpersist cached data	The problem might be reproduced in spark-shell with following code snippet:  import org.apache.spark.SparkContext  import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS  import org.apache.spark.mllib.linalg.Vectors  import org.apache.spark.mllib.regression.LabeledPoint    val samples = Seq[LabeledPoint](    LabeledPoint(1.0, Vectors.dense(1.0, 0.0)),    LabeledPoint(1.0, Vectors.dense(0.0, 1.0)),    LabeledPoint(0.0, Vectors.dense(1.0, 1.0)),    LabeledPoint(0.0, Vectors.dense(0.0, 0.0))  )    val rdd = sc.parallelize(samples)    for (i <- 0 until 10) {    val model = {      new LogisticRegressionWithLBFGS()        .setNumClasses(2)        .run(rdd)        .clearThreshold()    }  }  After code execution there are 10 MapPartitionsRDD objects on "Storage" tab in Spark application UI.
Spark SQL does not handle comma separates paths on Hadoop FileSystem	Spark SQL uses a Map[String, String] for data source settings. As a consequence the only way to pass in multiple paths (something that hadoop file input format supports) is to do pass in a comma separated list. For example:  sqlContext.format("json").load("dir1,dir22")  or  sqlContext.format("json").option("path", "dir1,dir2").load  However in this case ResolvedDataSource does not handle the comma delimited paths correctly for a HadoopFsRelationProvider. It treats the multiple comma delimited paths as single path.  For example if i pass in for path "dir1,dir2" it will make dir1 qualified but ignore dir2 (presumably because it simply treats it as part of dir1). If globs are involved then it simply always returns an empty array of paths (because the glob with comma in it doesn¡¯t match anything).  I think its important to handle commas to pass in multiple paths, since the framework does not provide an alternative. In some cases like parquet the code simply bypasses ResolvedDataSource to support multiple paths but to me this is a workaround that should be discouraged.
Some internal spark classes are not registered with kryo	When running a job using kryo serialization and setting `spark.kryo.registrationRequired=true` some internal classes are not registered, causing the job to die. This is still a problem when this setting is false (which is the default) because it makes the space required to store serialized objects in memory or disk much much more expensive in terms of runtime and storage space. 15/08/25 20:28:21 WARN spark.scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, a.b.c.d): java.lang.IllegalArgumentException: Class is not registered: scala.Tuple2[] Note: To register this class use: kryo.register(scala.Tuple2[].class);         at com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:442)         at com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:79)         at com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:472)         at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:565)         at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:250)         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:236)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)         at java.lang.Thread.run(Thread.java:745)
respect non-deterministic expressions in PhysicalOperation	We did a lot of special handling for non-deterministic expressions in Optimizer. However, PhysicalOperation just collects all Projects and Filters and messed it up. We should respect the operators order caused by non-deterministic expressions in PhysicalOperation.
Cache Table is not working while subquery has alias in its project list	Code to reproduce that:     import org.apache.spark.sql.hive.execution.HiveTableScan     sql("select key, value, key + 1 from src").registerTempTable("abc")     cacheTable("abc")      val sparkPlan = sql(       """select a.key, b.key, c.key from         |abc a join abc b on a.key=b.key         |join abc c on a.key=c.key""".stripMargin).queryExecution.sparkPlan      assert(sparkPlan.collect { case e: InMemoryColumnarTableScan => e }.size === 3) // failed     assert(sparkPlan.collect { case e: HiveTableScan => e }.size === 0) // failed The query plan like: == Parsed Logical Plan == 'Project [unresolvedalias('a.key),unresolvedalias('b.key),unresolvedalias('c.key)]  'Join Inner, Some(('a.key = 'c.key))   'Join Inner, Some(('a.key = 'b.key))    'UnresolvedRelation [abc], Some(a)    'UnresolvedRelation [abc], Some(b)   'UnresolvedRelation [abc], Some(c)  == Analyzed Logical Plan == key: int, key: int, key: int Project [key#14,key#61,key#66]  Join Inner, Some((key#14 = key#66))   Join Inner, Some((key#14 = key#61))    Subquery a     Subquery abc      Project [key#14,value#15,(key#14 + 1) AS _c2#16]       MetastoreRelation default, src, None    Subquery b     Subquery abc      Project [key#61,value#62,(key#61 + 1) AS _c2#58]       MetastoreRelation default, src, None   Subquery c    Subquery abc     Project [key#66,value#67,(key#66 + 1) AS _c2#63]      MetastoreRelation default, src, None  == Optimized Logical Plan == Project [key#14,key#61,key#66]  Join Inner, Some((key#14 = key#66))   Project [key#14,key#61]    Join Inner, Some((key#14 = key#61))     Project [key#14]      InMemoryRelation [key#14,value#15,_c2#16], true, 10000, StorageLevel(true, true, false, true, 1), (Project [key#14,value#15,(key#14 + 1) AS _c2#16]), Some(abc)     Project [key#61]      MetastoreRelation default, src, None   Project [key#66]    MetastoreRelation default, src, None  == Physical Plan == TungstenProject [key#14,key#61,key#66]  BroadcastHashJoin [key#14], [key#66], BuildRight   TungstenProject [key#14,key#61]    BroadcastHashJoin [key#14], [key#61], BuildRight     ConvertToUnsafe      InMemoryColumnarTableScan [key#14], (InMemoryRelation [key#14,value#15,_c2#16], true, 10000, StorageLevel(true, true, false, true, 1), (Project [key#14,value#15,(key#14 + 1) AS _c2#16]), Some(abc))     ConvertToUnsafe      HiveTableScan [key#61], (MetastoreRelation default, src, None)   ConvertToUnsafe    HiveTableScan [key#66], (MetastoreRelation default, src, None)
spark-submit to yarn doesn't fail if executors is 0	Running spark-submit with yarn with number-executors equal to 0 when not using dynamic allocation should error out. In spark 1.5.0 it continues and ends up hanging. yarn.ClientArguments still has the check so something else must have changed. spark-submit --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi --num-executors 0 .... spark 1.4.1 errors with: java.lang.IllegalArgumentException: Number of executors was 0, but must be at least 1 (or 0 if dynamic executor allocation is enabled).
Views are broken	I haven't dug into this yet... but it seems like this should work: This works: SELECT * FROM 100milints This seems to work: CREATE VIEW testView AS SELECT * FROM 100milints This fails: SELECT * FROM testView  org.apache.spark.sql.AnalysisException: cannot resolve '100milints.col' given input columns id; line 1 pos 7  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:56)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:53)  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:293)  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:292)  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:290)  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)  at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)  at scala.collection.Iterator$class.foreach(Iterator.scala:727)  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)  at scala.collection.AbstractIterator.to(Iterator.scala:1157)  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)  at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:290)  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:108)  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:118)  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:122)  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)  at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)  at scala.collection.AbstractTraversable.map(Traversable.scala:105)  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:122)  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)  at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)  at scala.collection.Iterator$class.foreach(Iterator.scala:727)  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)  at scala.collection.AbstractIterator.to(Iterator.scala:1157)  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)  at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:126)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:53)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:49)  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:103)  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)  at scala.collection.immutable.List.foreach(List.scala:318)  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:102)  at scala.collection.immutable.List.foreach(List.scala:318)  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:102)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:49)  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)  at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:908)  at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:132)  at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)
UnsafeRow.getUTF8String should handle off-heap backed UnsafeRow	UnsafeRow.getUTF8String delegates to UTF8String.fromAddress which returns null when passed a null base object, failing to handle off-heap backed {{UnsafeRow}}s correctly. This will also cause a NullPointerException when getString is called with off-heap storage.
Iterating through Column results in infinite loop	Iterating through a Column object results in an infinite loop. df = sqlContext.jsonRDD(sc.parallelize(['{"name": "El Magnifico"}'])) for i in df["name"]: print i Result: Column<name[0]> Column<name[1]> Column<name[2]> Column<name[3]> ¡­
AccumulableInfo and RDDOperationScope violates hashCode + equals contract	hashCode implementation is missing in classes AccumulableInfo and RDDOperationScope which are overriding equals methods
Support aggregation expressions in Order By	Followup on SPARK-6583  The following still fails.  val df = sqlContext.read.json("examples/src/main/resources/people.json")  df.registerTempTable("t")  val df2 = sqlContext.sql("select age, count(*) from t group by age order by count(*)")  df2.show()  StackTrace  Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: No function to evaluate expression. type: Count, tree: COUNT(1)   at org.apache.spark.sql.catalyst.expressions.AggregateExpression.eval(aggregates.scala:41)   at org.apache.spark.sql.catalyst.expressions.RowOrdering.compare(rows.scala:219)  In 1.4 the issue seemed to be BindReferences.bindReference didn't handle this case.  Haven't looked at 1.5 code, but don't see a change to bindReference in this patch.
select cast('false' as boolean) returns true	
UserDefinedType.typeName gives wrong result	DataType.typeName is defined as:    /** Name of the type used in JSON serialization. */    def typeName: String = this.getClass.getSimpleName.stripSuffix("$").dropRight(4).toLowerCase  The dropRight(4) is used to drop the Type suffix. However, UDTs may not have such suffix. For example, NestedStructUDT.typeName gives nestedstruc.
LogisticRegression copy should copy model summary if available	SPARK-9112 adds LogisticRegressionSummary but does not copy the model summary if available  We should add behavior similar to that in LinearRegression.copy
ML.LinearRegressionModel.copy() can not use argument "extra"	ML.LinearRegressionModel.copy() ignored argument extra, it will not take effect when users setting this parameter.
Release utils does not work with new version of jira-python library	Location of JIRAError has moved between old and new versions of python-jira package.
Minimum ratio of registered resources [ spark.scheduler.minRegisteredResourcesRatio] is not enabled for Mesos Coarse Grained mode	"spark.scheduler.minRegisteredResourcesRatio" configuration parameter is not effecting the Mesos Coarse Grained mode. This is because the scheduler is not overriding the "sufficientResourcesRegistered" function which is true by default.
Wrong executor state in standalone master because of wrong state transition	Because of concurrency issue in executor state transition, the executor state saved in standalone may possibly be LOADING rather than RUNNING. This is because of RUNNING state is delivered earlier than LOADING.  We have to guarantee the correct state changing, like: LAUNCHING -> LOADING -> RUNNING.
Add defense against external site framing	This came up as a minor point during a security audit using a common scanning tool: It's best if Spark UIs try to actively defend against certain types of frame-related vulnerabilities by setting X-Frame-Options. See https://www.owasp.org/index.php/Clickjacking_Defense_Cheat_Sheet  Easy PR coming ...
ApplicationMaster "--help" references the removed "--num-executors" option	The issue SPARK-9092 and commit 738f35 removed the ApplicationMaster commandline argument --num-executors, but it's help message still references the argument.
RoutingTablePartition toMessage method refers to bytes instead of bits	
Race condition between scheduler and YARN executor status update	This is a follow up to SPARK-8167. From the comment left in the code:  TODO there's a race condition where while we are querying the ApplicationMaster for the executor loss reason, there is the potential that tasks will be scheduled on the executor that failed. We should fix this by having this onDisconnected event also "blacklist" executors so that tasks are not assigned to them.
select(df(*)) fails when a column has special characters	Best explained with this example:  val df = sqlContext.read.json(sqlContext.sparkContext.makeRDD(        """{"a.b": "c", "d": "e" }""" :: Nil))  df.select("*").show()     //successful  df.select(df("*")).show() //throws exception  df.withColumnRenamed("d", "f").show() //also fails, possibly related
Use properties from ActiveJob associated with a Stage	This issue was addressed in #5494, but the fix in that PR, while safe in the sense that it will prevent the SparkContext from shutting down, misses the actual bug. The intent of submitMissingTasks should be understood as "submit the Tasks that are missing for the Stage, and run them as part of the ActiveJob identified by jobId". Because of a long-standing bug, the jobId parameter was never being used. Instead, we were trying to use the jobId with which the Stage was created ¨C which may no longer exist as an ActiveJob, hence the crash reported in SPARK-6880.  The correct fix is to use the ActiveJob specified by the supplied jobId parameter, which is guaranteed to exist at the call sites of submitMissingTasks.  This fix should be applied to all maintenance branches, since it has existed since 1.0.  Tasks for a Stage that was previously part of a Job that is no longer active would be re-submitted as though they were part of the prior Job and with no properties set. Since properties are what are used to set an other-than-default scheduling pool, this would affect FAIR scheduler usage, but it would also affect anything else that depends on the settings of the properties (which would be just user code at this point, since Spark itself doesn't really use the properties for anything else other than Job Group and Description, which end up in the WebUI, can be used to kill by JobGroup, etc.) Even the default, FIFO scheduling would be affected, however, since the resubmission of the Tasks under the earlier jobId would effectively give them a higher priority/greater urgency than the ActiveJob that now actually needs them. In any event, the Tasks would generate correct results.
Calling a UDF with insufficient number of input arguments should throw an analysis error	import org.apache.spark.sql.functions._  Seq((1,2)).toDF("a", "b").select(callUDF("percentile", $"a"))  This should throws an Analysis Exception.
javax.jdo.JDOFatalUserException in executor	HadoopRDD throws exception in executor, something like below.  5/09/17 18:51:21 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore  15/09/17 18:51:21 INFO metastore.ObjectStore: ObjectStore, initialize called  15/09/17 18:51:21 WARN metastore.HiveMetaStore: Retrying creating default database after error: Class org.datanucleus.api.jdo.JDOPersistenceManagerFactory was not found.  javax.jdo.JDOFatalUserException: Class org.datanucleus.api.jdo.JDOPersistenceManagerFactory was not found.   at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1175)   at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)   at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)   at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)   at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)   at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)   at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)   at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)   at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)   at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)   at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)   at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)   at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)   at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)   at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)   at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)   at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)   at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)   at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)   at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)   at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)   at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)   at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)   at java.lang.reflect.Constructor.newInstance(Constructor.java:526)   at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)   at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)   at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)   at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)   at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)   at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)   at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)   at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)   at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)   at org.apache.hadoop.hive.ql.plan.PlanUtils.configureJobPropertiesForStorageHandler(PlanUtils.java:803)   at org.apache.hadoop.hive.ql.plan.PlanUtils.configureInputJobPropertiesForStorageHandler(PlanUtils.java:782)   at org.apache.spark.sql.hive.HadoopTableReader$.initializeLocalJobConfFunc(TableReader.scala:298)   at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$12.apply(TableReader.scala:274)   at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$12.apply(TableReader.scala:274)   at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176)   at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:176)   at scala.Option.map(Option.scala:145)   at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:176)   at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:220)   at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216)   at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)   at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)   at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)   at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)   at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)   at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)   at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)   at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)   at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)   at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)   at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)   at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)   at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)   at org.apache.spark.scheduler.Task.run(Task.scala:88)   at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)   at java.lang.Thread.run(Thread.java:745)
Stop converting internal rows to external rows in DataFrame.toJSON	DataFrame.toJSON uses DataFrame.mapPartitions, which converts internal rows to external rows. We can use queryExecution.toRdd.mapPartitions instead for better performance.  Another issue is that, for UDT values, serialize produces internal types. So currently we must deal with both internal and external types within toJSON (see here), which is pretty weird.
Set operation output columns may have incorrect nullability	If "dual" is a one-row table, the query  SELECT count(v) FROM (   SELECT v FROM (   SELECT NULL AS v FROM dual   UNION ALL   SELECT 'foo' AS v FROM dual  ) my_union WHERE isnull(v)  ) my_subview;  returns 0 and the same query with union'ed sub-queries switched returns 1:  SELECT count(v) FROM (   SELECT v FROM (   SELECT 'foo' AS v FROM dual  UNION ALL   SELECT NULL AS v FROM dual   ) my_union WHERE isnull(v)  ) my_subview;  Example output (with Catalyst tracing turned on): https://gist.githubusercontent.com/mbautin/c916a2a7ce733d039137/raw  This is caused by the behavior in set operation implementation where only the nullability of the first child is taken into account:  SetOperation in org.apache.spark.sql.catalyst.plans.logical:  https://github.com/apache/spark/blob/82268f07abfa658869df2354ae72f8d6ddd119e8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicOperators.scala#L93  Union in org.apache.spark.sql.execution:  https://github.com/apache/spark/blob/e626ac5f5c27dcc74113070f2fec03682bcd12bd/sql/core/src/main/scala/org/apache/spark/sql/execution/basicOperators.scala#L178  Intersect in org.apache.spark.sql.execution:  https://github.com/apache/spark/blob/e626ac5f5c27dcc74113070f2fec03682bcd12bd/sql/core/src/main/scala/org/apache/spark/sql/execution/basicOperators.scala#L320  Except in org.apache.spark.sql.execution:  https://github.com/apache/spark/blob/e626ac5f5c27dcc74113070f2fec03682bcd12bd/sql/core/src/main/scala/org/apache/spark/sql/execution/basicOperators.scala#L307
ML Param validate should print better error information	Currently when you set illegal value for params of array type (such as IntArrayParam, DoubleArrayParam, StringArrayParam), it will throw IllegalArgumentException but with incomprehensible error information.  For example:  val vectorSlicer = new VectorSlicer().setInputCol("features").setOutputCol("result")  vectorSlicer.setIndices(Array.empty).setNames(Array("f1", "f4", "f1"))  It will throw IllegalArgumentException as:  vectorSlicer_b3b4d1a10f43 parameter names given invalid value [Ljava.lang.String;@798256c5.  java.lang.IllegalArgumentException: vectorSlicer_b3b4d1a10f43 parameter names given invalid value [Ljava.lang.String;@798256c5.  Users can not understand which params were set incorrectly.
NullPointerException when transform function in DStream returns NULL	NullPointerException raises when transform function returns NULL:  java.lang.NullPointerException  at org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$3.apply(DStream.scala:442)  at org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$3.apply(DStream.scala:441)  at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:107)  at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:107)  at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)  at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:107)  at org.apache.spark.streaming.dstream.DStream.clearMetadata(DStream.scala:441)  at org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$5.apply(DStream.scala:454)  at org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$5.apply(DStream.scala:454)  at scala.collection.immutable.List.foreach(List.scala:318)  at org.apache.spark.streaming.dstream.DStream.clearMetadata(DStream.scala:454)  at org.apache.spark.streaming.DStreamGraph$$anonfun$clearMetadata$2.apply(DStreamGraph.scala:129)  at org.apache.spark.streaming.DStreamGraph$$anonfun$clearMetadata$2.apply(DStreamGraph.scala:129)  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)  at org.apache.spark.streaming.DStreamGraph.clearMetadata(DStreamGraph.scala:129)  at org.apache.spark.streaming.scheduler.JobGenerator.clearMetadata(JobGenerator.scala:257)  at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:178)  at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:83)  at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:82)  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)  The code is very simple:          val sc = new SparkContext(conf)          val sqlContext = new HiveContext(sc)          import sqlContext.implicits._          println(">>> create streamingContext.")          val ssc = new StreamingContext(sc, Seconds(1))          ssc.queueStream(              Queue(                  sc.makeRDD(Seq(1)),                   sc.makeRDD(Seq[Int]()),                   sc.makeRDD(Seq(2))                  ), true).transform(rdd => if (rdd.isEmpty()) rdd else null).print          ssc.start()
SparkSQLCLIDriver should take the whole statement to generate the CommandProcessor	In the now implementation of SparkSQLCLIDriver.scala:   val proc: CommandProcessor = CommandProcessorFactory.get(Array(tokens(0)), hconf)  CommandProcessorFactory only take the first token of the statement, and this will be hard to diff the statement delete jar xxx and delete from xxx.  So maybe it's better to take the whole statement into the CommandProcessorFactory.
AppClient should not use `askWithReply` in `receiveAndReply` directly	`askWithReply` is a blocking action and it could block the message loop. We should not call it in `receiveAndReply` directly. It's better to use them in a separate thread pool.
Exception not failing R applications (in yarn cluster mode)	The bug is the R version of SPARK-7736. The R script fails with an exception but the Yarn application status is SUCCEEDED.
[Spark SQL] [UDF] the ceil/ceiling function got wrong return value type	As per ceil/ceiling definition,it should get BIGINT return value  -ceil(DOUBLE a), ceiling(DOUBLE a)  -Returns the minimum BIGINT value that is equal to or greater than a.  But in current Spark implementation, it got wrong value type.  e.g.,   select ceil(2642.12) from udf_test_web_sales limit 1;  2643.0  In hive implementation, it got return value type like below:  hive> select ceil(2642.12) from udf_test_web_sales limit 1;  OK  2643
RowMatrix.computeCovariance() result is not exactly symmetric	For some matrices, I have seen that the computed covariance matrix is not exactly symmetric, most likely due to some numerical rounding errors. This is problematic when trying to construct an instance of MultivariateGaussian, because it requires an exactly symmetric covariance matrix. See reproducible example below.  I would suggest modifying the implementation so that G(i, j) and G(j, i) are set at the same time, with the same value.  val rdd = RandomRDDs.normalVectorRDD(sc, 100, 10, 0, 0)  val matrix = new RowMatrix(rdd)  val mean = matrix.computeColumnSummaryStatistics().mean  val cov = matrix.computeCovariance()  val dist = new MultivariateGaussian(mean, cov) //throws breeze.linalg.MatrixNotSymmetricException
YARN executors are launched with the default perm gen size	Unlike other backends, the YARN one does not explicitly set the perm gen size for the executor process. That means that, unless the user has explicitly changed it by adding extra java options, executors on YARN are running with 64m of perm gen (I believe) instead of 256m like the other backends.
ClassNotFoundException when running on Client mode, with a Mesos master.	When running an example task on a Mesos cluster (local master, local agent), any Spark tasks will stall with the following error (in the executor's stderr):  Works fine in coarse-grained mode, only fails in fine-grained mode.  15/10/07 15:21:14 INFO Utils: Successfully started service 'sparkExecutor' on port 53689.  15/10/07 15:21:14 WARN TransportChannelHandler: Exception in connection from /10.0.79.8:53673  java.lang.ClassNotFoundException: org/apache/spark/rpc/netty/AskResponse   at java.lang.Class.forName0(Native Method)   at java.lang.Class.forName(Class.java:348)   at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)   at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)   at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)   at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)   at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)   at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)   at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)   at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)   at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:227)   at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)   at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:265)   at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:226)   at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$3$$anon$4.onSuccess(NettyRpcEnv.scala:196)   at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:152)   at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:103)   at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)   at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)   at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)   at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)   at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)   at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)   at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)   at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)   at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)   at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)   at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)   at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)   at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)   at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)   at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)   at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)   at java.lang.Thread.run(Thread.java:745)  15/10/07 15:21:14 ERROR NettyRpcHandler: org/apache/spark/rpc/netty/AskResponse  java.lang.ClassNotFoundException: org/apache/spark/rpc/netty/AskResponse   at java.lang.Class.forName0(Native Method)   at java.lang.Class.forName(Class.java:348)   at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)   at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)   at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)   at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)   at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)   at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)   at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)   at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)   at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$deserialize$1.apply(NettyRpcEnv.scala:227)   at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)   at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:265)   at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:226)   at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$3$$anon$4.onSuccess(NettyRpcEnv.scala:196)   at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:152)   at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:103)   at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)   at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)   at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)   at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)   at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)   at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)   at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)   at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)   at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)   at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)   at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)   at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)   at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)   at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)   at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)   at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)   at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)   at java.lang.Thread.run(Thread.java:745)  The commands to setup the environment and submit a job:  cd <mesos-binaries>  mesos-master.sh --ip=127.0.0.1 --work_dir=<temp>  mesos-slave.sh --master=127.0.0.1:5050    cd <spark-home>  bin/spark-submit --master mesos://127.0.0.1:5050 examples/src/main/python/pi.py 10  On v1.4.1 and v1.5.1, the Spark job finished with no problems.  Possibly introduced by SPARK-6028.
yarn-client mode misbehaving with netty-based RPC backend	YARN running in cluster deploy mode seems to be having issues with the new RPC backend; if you look at unit test runs, tests that run in cluster mode are taking several minutes to run, instead of the more usual 20-30 seconds.  For example, https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/43349/consoleFull:  [info] YarnClusterSuite:  [info] - run Spark in yarn-client mode (13 seconds, 953 milliseconds)  [info] - run Spark in yarn-cluster mode (6 minutes, 50 seconds)  [info] - run Spark in yarn-cluster mode unsuccessfully (1 minute, 53 seconds)  [info] - run Python application in yarn-client mode (21 seconds, 842 milliseconds)  [info] - run Python application in yarn-cluster mode (7 minutes, 0 seconds)  [info] - user class path first in client mode (1 minute, 58 seconds)  [info] - user class path first in cluster mode (4 minutes, 49 seconds)
Show non-children in default Expression.toString	
Add dictionary support for CatalystDecimalConverter	Currently CatalystDecimalConverter doesn't explicitly support dictionary encoding. The consequence is that, the underlying Parquet ColumnReader always sends raw Int/Long/Binary values decoded from the dictionary to CatalystDecimalConverter even if the column is encoded using a dictionary. By adding explicit dictionary support (similar to what CatalystStringConverter does), we can avoid constructing decimals repeatedly. This should be especially effective for binary backed decimals.
Spark fails when running with a task that requires a more recent version of RoaringBitmaps	The following error appears during Kryo init whenever a more recent version (>0.5.0) of Roaring bitmaps is required by a job. org/roaringbitmap/RoaringArray$Element was removed in 0.5.0  A needed class was not found. This could be due to an error in your runpath. Missing class: org/roaringbitmap/RoaringArray$Element  java.lang.NoClassDefFoundError: org/roaringbitmap/RoaringArray$Element   at org.apache.spark.serializer.KryoSerializer$.<init>(KryoSerializer.scala:338)   at org.apache.spark.serializer.KryoSerializer$.<clinit>(KryoSerializer.scala)   at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:93)   at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:237)   at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:222)   at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:138)   at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:201)   at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:102)   at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:85)   at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)   at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:63)   at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1318)   at org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply(SparkContext.scala:1006)   at org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply(SparkContext.scala:1003)   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)   at org.apache.spark.SparkContext.withScope(SparkContext.scala:700)   at org.apache.spark.SparkContext.hadoopFile(SparkContext.scala:1003)   at org.apache.spark.SparkContext$$anonfun$textFile$1.apply(SparkContext.scala:818)   at org.apache.spark.SparkContext$$anonfun$textFile$1.apply(SparkContext.scala:816)   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)   at org.apache.spark.SparkContext.withScope(SparkContext.scala:700)   at org.apache.spark.SparkContext.textFile(SparkContext.scala:816)  See https://issues.apache.org/jira/browse/SPARK-5949 for related info
[streaming] [flume] Gracefully shutdown Flume receiver threads	Currently, we kill the executor fetching data from Flume by interrupting the threads. We should wait for a bit before interrupting the threads. This is more graceful and we avoid some nasty exceptions in the logs
HistoryServer fails to come up if HDFS takes too long to come out of safe mode	When HDFS is starting up, it starts in safe mode until the NN is able to read the whole fs image and initialize everything. For a really large NN that can take a while.  If the HS is started at the same time, it may give up trying to check whether the event log directory exists, and exit. That's a little sub-optimal; the HS could wait until HDFS came out of safe mode instead.
Optimize NULL in <inlist-expressions> by folding it to Literal(null)	Add a rule in optimizer to convert NULL [NOT] IN (expr1,...,expr2) to  Literal(null).  This is a follow up defect to SPARK-8654 and suggested by Wenchen Fan.
SaslRpcHandler does not delegate all methods to underlying handler	SaslRpcHandler only delegates receive and getStreamManager, so when SASL is enabled, other events will be missed by apps.  This affects other version too, but I think these events aren't actually used there. They'll be used by the new rpc backend in 1.6, though.
Hive Thrift Server will log warn "Couldn't find log associated with operation handle"	The warnning log is below:  Warnning Log  15/10/09 16:48:23 WARN thrift.ThriftCLIService: Error fetching results:   org.apache.hive.service.cli.HiveSQLException: Couldn't find log associated with operation handle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=fb0900c7-6244-432e-a779-b449ca7f7ca0]   at org.apache.hive.service.cli.operation.OperationManager.getOperationLogRowSet(OperationManager.java:229)   at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:687)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:497)   at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)   at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)   at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)   at java.security.AccessController.doPrivileged(Native Method)   at javax.security.auth.Subject.doAs(Subject.java:422)   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)   at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)   at com.sun.proxy.$Proxy32.fetchResults(Unknown Source)   at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:454)   at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:672)   at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1553)   at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1538)   at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)   at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)   at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)   at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)   at java.lang.Thread.run(Thread.java:745)  Once I execute a statement, there will have this warnning log by the default configuration.
Fix some potential NPEs in DStream transformation	Guard out some potential NPEs when input stream returns None instead of empty RDD.
Incorporate per-JVM id into ExprId to prevent unsafe cross-JVM comparisions	In the current implementation of named expressions' ExprIds, we rely on a per-JVM AtomicLong to ensure that expression ids are unique within a JVM. However, these expression ids will not be globally unique. This opens the potential for id collisions if new expression ids happen to be created inside of tasks rather than on the driver.  There are currently a few cases where tasks allocate expression ids, which happen to be safe because those expressions are never compared to expressions created on the driver. In order to guard against the introduction of invalid comparisons between driver-created and executor-created expression ids, this patch extends ExprId to incorporate a UUID to identify the JVM that created the id, which prevents collisions.
Optimize DataSourceStrategy.mergeWithPartitionValues	This method is essentially a projection, but it's implemented in an pretty inefficient way and causes significant boxing cost.
ChildFirstURLClassLoader#getResources should return all found resources, not just those in the child classloader	Currently when using a child-first classloader (spark.driver|executor.userClassPathFirst = true), the getResources method does not return any matching resources from the parent classloader if the child classloader contains any. This is not child-first, it's child-only and is inconsistent with how the default parent-first classloaders work in the JDK (all found resources are returned from both classloaders). It is also inconsistent with how child-first classloaders work in other environments (Servlet containers, for example).   ChildFirstURLClassLoader#getResources() should return resources found from both the child and the parent classloaders, placing any found from the child classloader first.  For reference, the specific use case where I encountered this problem was running Spark on AWS EMR in a child-first arrangement (due to guava version conflicts), where Akka's configuration file (reference.conf) was made available in the parent classloader, but was not visible to the Typesafe config library which uses Classloader.getResources() on the Thread's context classloader to find them. This resulted in a fatal error from the Config library: "com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'akka.version'" .
Disitribute the log4j.properties files from the client to the executors	The log4j.properties file from the client is not distributed to the executors. This means that the client settings are not applied to the executors and they run with the default settings.  This affects troubleshooting and data gathering.  The workaround is to use the --files option for spark-submit to propagate the log4j.properties file
maxNumExecutorFailures defaults to 3 under dynamic allocation	With dynamic allocation, the spark.executor.instances config is 0, meaning this line ends up with maxNumExecutorFailures equal to 3, which for me has resulted in large dynamicAllocation jobs with hundreds of executors dying due to one bad node serially failing executors that are allocated on it. I think that using spark.dynamicAllocation.maxExecutors would make most sense in this case; I frequently run shells that vary between 1 and 1000 executors, so using s.dA.minExecutors or s.dA.initialExecutors would still leave me with a value that is lower than makes sense.
Incorrect TaskLocation type	"toString" is the only difference between HostTaskLocation and HDFSCacheTaskLocation for the moment, but it would be better to correct this.
JsonParser/Generator should be closed for resource recycle	Some json parsers are not closed. parser in JacksonParser#parseJson, for example.
Worker registration protocol is racy	I ran into this while making changes to the new RPC framework. Because the Worker registration protocol is based on sending unrelated messages between Master and Worker, it's possible that another message (e.g. caused by an a app trying to allocate workers) to arrive at the Worker before it knows the Master has registered it. This triggers the following code:     case LaunchExecutor(masterUrl, appId, execId, appDesc, cores_, memory_) =>       if (masterUrl != activeMasterUrl) {         logWarning("Invalid Master (" + masterUrl + ") attempted to launch executor.") This may or may not be made worse by SPARK-11098. A simple workaround is to use an ask instead of a send for these messages. That should at least narrow the race. Note this is more of a problem in local-cluster mode, used a lot by unit tests, where Master and Worker instances are coming up as part of the app itself.
Logging trait should be private - not DeveloperApi	The classdoc says:  * NOTE: DO NOT USE this class outside of Spark. It is intended as an internal utility.  *       This will likely be changed or removed in future releases.
Spark 1.5+ Kinesis Streaming - ClassCastException when starting KinesisReceiver	After upgrading from Spark 1.4.x -> 1.5.x, I am now unable to start a Kinesis Spark Streaming application, and am being consistently greeted with this exception: java.lang.ClassCastException: scala.collection.mutable.HashMap cannot be cast to scala.collection.mutable.SynchronizedMap at org.apache.spark.streaming.kinesis.KinesisReceiver.onStart(KinesisReceiver.scala:175) at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:148) at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:130) at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:542) at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:532) at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:1982) at org.apache.spark.SparkContext$$anonfun$38.apply(SparkContext.scala:1982) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:88) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Worth noting that I am able to reproduce this issue locally, and also on Amazon EMR (using the latest emr-release 4.1.0 which packages Spark 1.5.0). Also, I am not able to run the included kinesis-asl example. Built locally using: git checkout v1.5.1 mvn -Pyarn -Pkinesis-asl -Phadoop-2.6 -DskipTests clean package Example run command: bin/run-example streaming.KinesisWordCountASL phibit-test kinesis-connector https://kinesis.us-east-1.amazonaws.com
Filter out 'hive.metastore.rawstore.impl' from executionHive temporary config	Spark use two hive meta stores: external one for storing tables and internal one (executionHive): /** The copy of the hive client that is used for execution. Currently this must always be Hive 13 as this is the version of Hive that is packaged with Spark SQL. This copy of the client is used for execution related tasks like registering temporary functions or ensuring that the ThreadLocal SessionState is correctly populated. This copy of Hive is not used for storing persistent metadata, and only point to a dummy metastore in a temporary directory. */ The executionHive assumed to be a standard meta store located in temporary directory as a derby db. But hive.metastore.rawstore.impl was not filtered out so any custom implementation of the metastore with other storage properties (not JDO) will persist that temporary functions. CassandraMetaStore from DataStax Enterprise is one of examples.
`./sbin/start-slave.sh --help` should print out the help message	Reading the sources has showed that the command ./sbin/start-slave.sh --help should print out the help message. It doesn't really. ?  spark git:(master) ? ./sbin/start-slave.sh --help starting org.apache.spark.deploy.worker.Worker, logging to /Users/jacek/dev/oss/spark/sbin/../logs/spark-jacek-org.apache.spark.deploy.worker.Worker-1-japila.local.out failed to launch org.apache.spark.deploy.worker.Worker:     --properties-file FILE   Path to a custom Spark properties file.                              Default is conf/spark-defaults.conf. full log in /Users/jacek/dev/oss/spark/sbin/../logs/spark-jacek-org.apache.spark.deploy.worker.Worker-1-japila.local.out
Fix R doc for DataFrame function `lit`	Currently the documentation for `lit` is inconsist with doc format, references Scala symbol and has no example.
ShuffleClient should release connection after fetching blocks had been completed in yarn's external shuffle	ExternalShuffleClient of executors reserve its connection with yarn's NodeManager until application has been completed. so it will make NodeManager has many socket connections. in order to reduce network pressure of NodeManager's shuffleService, after registerWithShuffleServer or fetchBlocks have been completed in ExternalShuffleClient, connection for shuffleService needs to be closed.
reset all accumulators in physical operators before execute an action	
YarnClient can't get tokens to talk to Hive 1.2.1 in a secure cluster	As reported on the dev list, trying to run a YARN client which wants to talk to Hive in a Kerberized hadoop cluster fails.
[SQL] Incorrect results when using rollup/cube	Spark SQL is unable to generate a correct result when the following query using rollup.  "select a, b, sum(a + b) as sumAB, GROUPING__ID from mytable group by a, b with rollup" Spark SQL generates a wrong result: [2,4,6,3] [2,null,null,1] [1,null,null,1] [null,null,null,0] [1,2,3,3] The table mytable is super simple, containing two rows and two columns: testData = Seq((1, 2), (2, 4)).toDF("a", "b") After turning off codegen, the query plan is like == Parsed Logical Plan == 'Rollup ['a,'b], unresolvedalias('a),unresolvedalias('b),unresolvedalias('sum(('a + 'b)) AS sumAB#20),unresolvedalias('GROUPING__ID) 'UnresolvedRelation `mytable`, None == Analyzed Logical Plan == a: int, b: int, sumAB: bigint, GROUPING__ID: int Aggregate a#2,b#3,grouping__id#23, a#2,b#3,sum(cast((a#2 + b#3) as bigint)) AS sumAB#20L,GROUPING__ID#23 Expand [0,1,3], a#2,b#3, grouping__id#23 Subquery mytable Project _1#0 AS a#2,_2#1 AS b#3 LocalRelation _1#0,_2#1, [[1,2],[2,4]] == Optimized Logical Plan == Aggregate a#2,b#3,grouping__id#23, a#2,b#3,sum(cast((a#2 + b#3) as bigint)) AS sumAB#20L,GROUPING__ID#23 Expand [0,1,3], a#2,b#3, grouping__id#23 LocalRelation a#2,b#3, [[1,2],[2,4]] == Physical Plan == Aggregate false, a#2,b#3,grouping__id#23, a#2,b#3,sum(PartialSum#24L) AS sumAB#20L,grouping__id#23 Exchange hashpartitioning(a#2,b#3,grouping__id#23,5) Aggregate true, a#2,b#3,grouping__id#23, a#2,b#3,grouping__id#23,sum(cast((a#2 + b#3) as bigint)) AS PartialSum#24L Expand List(null, null, 0),List(a#2, null, 1),List(a#2, b#3, 3), a#2,b#3,grouping__id#23 LocalTableScan a#2,b#3, [[1,2],[2,4]] Below are my observations: 1. Generation of GROUP__ID looks OK.  2. The problem still exists no matter whether turning on/off CODEGEN 3. Rollup still works in a simple query when group-by columns have only one column. For example, "select b, sum(a), GROUPING__ID from mytable group by b with rollup" 4. The buckets in "HiveDataFrameAnalytcisSuite" are misleading. Unfortunately, they hide the bugs. Although the buckets passed, they just compare the results of SQL and Dataframe. This way is unable to capture the regression when both return the same wrong results.  5. The same problem also exists in cube. I have not started the investigation in cube, but I believe the root causes should be the same.  6. It looks like all the logical plans are correct.
SizeEstimator prevents class unloading	The SizeEstimator keeps a cache of ClassInfos but this cache uses Class objects as keys. Which results in strong references to the Class objects. If these classes are dynamically created this prevents the corresponding ClassLoader from being GCed. Leading to PermGen exhaustion. An easy fix would be to use a WeakRef for the keys. A proposed fix can be found here: https://github.com/Site2Mobile/spark/commit/21c572cbda5607d0c7c6643bfaf43e53c8aa6f8c We are currently running this in production and it seems to resolve the issue. I will prepare a pull request ASAP.
sort_array throws exception scala.MatchError	I was trying out the sort_array function then hit this exception. I looked into the spark source code. I found the root cause is that sort_array does not check for an array of NULLs. It's not meaningful to sort an array of entirely NULLs anyway. Similar issue exists with an array of struct type.  I already have a fix for this issue and I'm going to create a pull request for it. scala> sqlContext.sql("select sort_array(array(null, null)) from t1").show() scala.MatchError: ArrayType(NullType,true) (of class org.apache.spark.sql.types.ArrayType) at org.apache.spark.sql.catalyst.expressions.SortArray.lt$lzycompute(collectionOperations.scala:68) at org.apache.spark.sql.catalyst.expressions.SortArray.lt(collectionOperations.scala:67) at org.apache.spark.sql.catalyst.expressions.SortArray.nullSafeEval(collectionOperations.scala:111) at org.apache.spark.sql.catalyst.expressions.BinaryExpression.eval(Expression.scala:341) at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$9$$anonfun$applyOrElse$2.applyOrElse(Optimizer.scala:440) at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$9$$anonfun$applyOrElse$2.applyOrElse(Optimizer.scala:433) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:227) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:227) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:226) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:232) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:232) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249) at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
Executing deploy.client TestClient fails with bad class name	Execution of deploy.client.TestClient creates an ApplicationDescription to start a TestExecutor which fails due to a bad class name. Currently it is "spark.deploy.client.TestExecutor" but should be "org.apache.spark.deploy.client.TestExecutor".
Reduce memory consumption of OutputCommitCoordinator bookkeeping structures	OutputCommitCoordinator uses a map in a place where an array would suffice, increasing its memory consumption for result stages with millions of tasks.
spark cannot describe temporary functions	create temporary function aa as ....; describe function aa; Will return 'Unable to find function aa', which is not right.
Keep full stack track in captured exception in PySpark	We should keep full stack trace in captured exception in PySpark.
HistoryPage not multi-tenancy enabled (app links not prefixed with APPLICATION_WEB_PROXY_BASE)	Links on HistoryPage are not prepended with uiRoot (export APPLICATION_WEB_PROXY_BASE=<uiRoot path>). This makes it impossible/unpractical to expose the History Server in a multi-tenancy environment where each Spark service instance has one history server behind a multi-tenant enabled proxy server. All other Spark web UI pages are correctly prefixed when the APPLICATION_WEB_PROXY_BASE environment variable is set. Repro steps: Configure history log collection: conf/spark-defaults.conf spark.eventLog.enabled         true spark.eventLog.dir             logs/history spark.history.fs.logDirectory  logs/history ...create the logs folders: $ mkdir -p logs/history Start the Spark shell and run the word count example: $ bin/spark-shell ... scala> sc.textFile("README.md").flatMap(_.split(" ")).map(w => (w, 1)).reduceByKey(_ + _).collect scala> sc.stop Set the web proxy root path path (i.e. /testwebuiproxy/..): $ export APPLICATION_WEB_PROXY_BASE=/testwebuiproxy/.. Start the history server: $  sbin/start-history-server.sh Bring up the History Server web UI at localhost:18080 and view the application link in the HTML source text:     ...     <thead><th width="" class="">App ID</th><th width="" class="">App Name</th>...</thead>       <tbody>         <tr>           <td><a href="/history/local-1445896187531">local-1445896187531</a></td><td>Spark shell</td>       ... Notice, application link "/history/local-1445896187531" does not have the prefix /testwebuiproxy/..   All site-relative links (URL starting with "/") should have been prepended with the uiRoot prefix /testwebuiproxy/.. like this ...     ...     <thead><th width="" class="">App ID</th><th width="" class="">App Name</th>...</thead>       <tbody>         <tr>           <td><a href="/testwebuiproxy/../history/local-1445896187531">local-1445896187531</a></td><td>Spark shell</td>
Support setting driver properties when starting Spark from R programmatically or from Rstudio	Currently when sparkR.init() is called in 'client' mode, it launches the JVM backend but driver properties (like driver-memory) are not passed or settable by the user calling sparkR.init(). Sun RuiShivaram Venkataraman and I discussed this offline and think we should support this. This is the original thread: >> From: rui.sun@intel.com >> To: dirceu.semighini@gmail.com >> CC: user@spark.apache.org >> Subject: RE: How to set memory for SparkR with master="local[*]" >> Date: Mon, 26 Oct 2015 02:24:00 +0000 >> >> As documented in >> http://spark.apache.org/docs/latest/configuration.html#available-prop >> e >> rties, >> >> Note for ¡°spark.driver.memory¡±: >> >> Note: In client mode, this config must not be set through the  >> SparkConf directly in your application, because the driver JVM has  >> already started at that point. Instead, please set this through the  >> --driver-memory command line option or in your default properties file. >> >> >> >> If you are to start a SparkR shell using bin/sparkR, then you can use  >> bin/sparkR ¨Cdriver-memory. You have no chance to set the driver  >> memory size after the R shell has been launched via bin/sparkR. >> >> >> >> Buf if you are to start a SparkR shell manually without using  >> bin/sparkR (for example, in Rstudio), you can: >> >> library(SparkR) >> >> Sys.setenv("SPARKR_SUBMIT_ARGS" = "--conf spark.driver.memory=2g >> sparkr-shell") >> >> sc <- sparkR.init() >> >> >> >> From: Dirceu Semighini Filho dirceu.semighini@gmail.com >> Sent: Friday, October 23, 2015 7:53 PM >> Cc: user >> Subject: Re: How to set memory for SparkR with master="local[*]" >> >> >> >> Hi Matej, >> >> I'm also using this and I'm having the same behavior here, my driver  >> has only 530mb which is the default value. >> >> >> >> Maybe this is a bug. >> >> >> >> 2015-10-23 9:43 GMT-02:00 Matej Holec <holecm@gmail.com>: >> >> Hello! >> >> How to adjust the memory settings properly for SparkR with master="local[*]" >> in R? >> >> >> When running from R ¨C SparkR doesn't accept memory settings  >> >> I use the following commands: >> >> R> library(SparkR) >> R> sc <- sparkR.init(master = "local[*]", sparkEnvir = >> list(spark.driver.memory = "5g")) >> >> Despite the variable spark.driver.memory is correctly set (checked in  >> http://node:4040/environment/), the driver has only the default  >> amount of memory allocated (Storage Memory 530.3 MB). >> >> But when running from spark-1.5.1-bin-hadoop2.6/bin/sparkR ¨C OK >> >> The following command: >> >> ]$ spark-1.5.1-bin-hadoop2.6/bin/sparkR --driver-memory 5g >> >> creates SparkR session with properly adjustest driver memory (Storage  >> Memory >> 2.6 GB). >> >> >> Any suggestion? >> >> Thanks >> Matej >> >>
Regression Imposes doubles on prediction/label columns	Using pyspark.ml and DataFrames, The ALS recommender cannot be evaluated using the RegressionEvaluator, because of a type mis-match between the model transformation and the evaluation APIs. One can work around this by casting the prediction column into double before passing it into the evaluator. However, this does not work with pipelines and cross validation. Code and traceback below: als = ALS(rank=10, maxIter=30, regParam=0.1, userCol='userID', itemCol='movieID', ratingCol='rating') model = als.fit(training) predictions = model.transform(validation) evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='rating') validationRmse = evaluator.evaluate(predictions, {evaluator.metricName: 'rmse'}) Traceback: validationRmse = evaluator.evaluate(predictions, {evaluator.metricName: 'rmse'} ) File "/Users/dominikdahlem/software/spark-1.6.0-SNAPSHOT-bin-custom-spark/python/lib/pyspark.zip/pyspark/ml/evaluation.py", line 63, in evaluate File "/Users/dominikdahlem/software/spark-1.6.0-SNAPSHOT-bin-custom-spark/python/lib/pyspark.zip/pyspark/ml/evaluation.py", line 94, in _evaluate File "/Users/dominikdahlem/software/spark-1.6.0-SNAPSHOT-bin-custom-spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py", line 813, in call File "/Users/dominikdahlem/projects/repositories/spark/python/pyspark/sql/utils.py", line 42, in deco raise IllegalArgumentException(s.split(': ', 1)[1]) pyspark.sql.utils.IllegalArgumentException: requirement failed: Column prediction must be of type DoubleType but was actually FloatType.
withNewChildren should not convert StructType to Seq	
StreamingContext.awaitTerminationOrTimeout does not return	The docs for SparkContext.awaitTerminationOrTimeout state it will "Return `true` if it's stopped; (...) or `false` if the waiting time elapsed before returning from the method." This is currently not the case - the function does not return and thus any logic built on awaitTerminationOrTimeout will not work.
unnecessary def dialectClassName in HiveContext, and misleading dialect conf at the start of spark-sql	1. def dialectClassName in HiveContext is unnecessary. In HiveContext, if conf.dialect == "hiveql", getSQLDialect() will return new HiveQLDialect(this); else it will use super.getSQLDialect(). Then in super.getSQLDialect(), it calls dialectClassName, which is overriden in HiveContext and still return super.dialectClassName. So we'll never reach the code "classOf[HiveQLDialect].getCanonicalName" of def dialectClassName in HiveContext. 2. When we start bin/spark-sql, the default context is HiveContext, and the corresponding dialect is hiveql. However, if we type "set spark.sql.dialect;", the result is "sql", which is inconsistent with the actual dialect and is misleading. For example, we can use sql like "create table" which is only allowed in hiveql, but this dialect conf shows it's "sql". Although this problem will not cause any execution error, it's misleading to spark sql users. Therefore I think we should fix it. In this pr, instead of overriding def dialect in conf of HiveContext, I set the SQLConf.DIALECT directly as "hiveql", such that result of "set spark.sql.dialect;" will be "hiveql", not "sql". After the change, we can still use "sql" as the dialect in HiveContext through "set spark.sql.dialect=sql". Then the conf.dialect in HiveContext will become sql. Because in SQLConf, def dialect = getConf(), and now the dialect in "settings" becomes "sql".
Add documentation on using SparkR from Rstudio	As per Shivaram Venkataraman we need to add a section in the programming guide on using SparkR from RStudio, in which we should talk about: how to load SparkR package what configurable options for initializing SparkR
Enable url link in R doc for Persist	
Forgot to update usage of "spark.sparkr.r.command" in RRDD in the PR for SPARK-10971	The PR for SPARK-10971 fixed core/src/main/scala/org/apache/spark/deploy/RRunner.scala to handle "spark.r.command" (with support for the depreciated "spark.sparkr.r.command"), but forgot to change core/src/main/scala/org/apache/spark/api/r/RRDD.scala, which still just uses "spark.sparkr.r.command". This should also be changed.
Personalized PageRank shouldn't use uniform initialization	The current implementation of personalized pagerank in GraphX uses uniform initialization over the full graph - every vertex will get initially activated. For example: import org.apache.spark._ import org.apache.spark.graphx._ import org.apache.spark.rdd.RDD val users: RDD[(VertexId, (String, String))] =   sc.parallelize(Array((3L, ("rxin", "student")), (7L, ("jgonzal", "postdoc")),                        (5L, ("franklin", "prof")), (2L, ("istoica", "prof")))) val relationships: RDD[Edge[String]] =   sc.parallelize(Array(Edge(3L, 7L, "collab"),    Edge(5L, 3L, "advisor"),                        Edge(2L, 5L, "colleague"), Edge(5L, 7L, "pi"))) val defaultUser = ("John Doe", "Missing") val graph = Graph(users, relationships, defaultUser) graph.staticPersonalizedPageRank(3L, 0, 0.15).vertices.collect.foreach(println) Leads to all vertices being set to resetProb (0.15), which is different from the behavior described in SPARK-5854, where only the source node should be activated. The risk is that, after a few iterations, the most activated nodes are the source node and the nodes that were untouched by the propagation. For example in the above example the vertex 2L will always have an activation of 0.15: graph.personalizedPageRank(3L, 0, 0.15).vertices.collect.foreach(println) Which leads into a higher score for 2L than for 7L and 5L, even though there's no outbound path from 3L to 2L.
Null comparison requires type information but type extraction fails for complex types	While comparing a Column to a null literal, comparison works only if type of null literal matches type of the Column it's being compared to. Example scala code (can be run from spark shell): import org.apache.spark.sql._ import org.apache.spark.sql.types._ import org.apache.spark.sql.catalyst.expressions._ val inputRowsData = Seq(Seq("abc"),Seq(null),Seq("xyz")) val inputRows = for(seq <- inputRowsData) yield Row.fromSeq(seq) val dfSchema = StructType(Seq(StructField("column", StringType, true))) val df = sqlContext.createDataFrame(sc.makeRDD(inputRows), dfSchema) //DOESN'T WORK val filteredDF = df.filter(df("column") <=> (new Column(Literal(null)))) //WORKS val filteredDF = df.filter(df("column") <=> (new Column(Literal.create(null, SparkleFunctions.dataType(df("column")))))) Why should type information be required for a null comparison? If it's required, it's not always possible to extract type information from complex types (e.g. StructType). Following scala code (can be run from spark shell), throws org.apache.spark.sql.catalyst.analysis.UnresolvedException: import org.apache.spark.sql._ import org.apache.spark.sql.types._ import org.apache.spark.sql.catalyst.expressions._ val inputRowsData = Seq(Seq(Row.fromSeq(Seq("abc", "def"))),Seq(Row.fromSeq(Seq(null, "123"))),Seq(Row.fromSeq(Seq("ghi", "jkl")))) val inputRows = for(seq <- inputRowsData) yield Row.fromSeq(seq) val dfSchema = StructType(Seq(StructField("column", StructType(Seq(StructField("p1", StringType, true), StructField("p2", StringType, true))), true))) val filteredDF = df.filter(df("column")("p1") <=> (new Column(Literal.create(null, SparkleFunctions.dataType(df("column")("p1")))))) org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: column#0[p1] at org.apache.spark.sql.catalyst.analysis.UnresolvedExtractValue.dataType(unresolved.scala:243) at org.apache.spark.sql.ArithmeticFunctions$class.dataType(ArithmeticFunctions.scala:76) at org.apache.spark.sql.SparkleFunctions$.dataType(SparkleFunctions.scala:14) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:43) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:45) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:47) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:49) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:51) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:53) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:55) at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:57) at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59) at $iwC$$iwC$$iwC$$iwC.<init>(<console>:61) at $iwC$$iwC$$iwC.<init>(<console>:63) at $iwC$$iwC.<init>(<console>:65) at $iwC.<init>(<console>:67) at <init>(<console>:69) at .<init>(<console>:73) at .<clinit>(<console>) at .<init>(<console>:7) at .<clinit>(<console>) at $print(<console>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819) at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857) at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902) at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814) at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657) at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665) at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945) at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059) at org.apache.spark.repl.Main$.main(Main.scala:31) at org.apache.spark.repl.Main.main(Main.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
append data to partitioned table will messes up the result	
Yarn AM proxy filter configuration should be reloaded when recovered from checkpoint	Currently Yarn AM proxy filter configuration is recovered from checkpoint file when Spark Streaming application is restarted, which will lead to some unwanted behaviors: 1. Wrong RM address if RM is redeployed from failure. 2. Wrong proxyBase, since app id is updated, old app id for proxyBase is wrong. So instead of recovering from checkpoint file, these configurations should be reloaded each time when app started. This problem only exists in Yarn cluster mode, for Yarn client mode, these configurations will be updated with RPC message AddWebUIFilter.
Wrong callsite is displayed when using AsyncRDDActions#takeAsync	When we call AsyncRDDActions#takeAsync, actually another DAGScheduler#runJob is called from another thread so we cannot get proper callsite infomation.
Giving precedence to proxyBase set by spark instead of env	Customer reported a strange UI when running spark application through Oozie in Uber mode (Issue was observed only in yarn-client mode). When debugging the sparkUI through chrome developer console, figured out that CSS files are looked for in different applicationId (Oozie mapreduce application) instead of actual spark application. (Please see the attached screenshot for more information). Looking into the live sparkUI code, it seems that proxyBase is taken from APPLICATION_WEB_PROXY_BASE instead of spark property spark.ui.proxyBase (Pointing to the actual spark application). This issue might be reproducible if the above specified env is set manually or set by other job. Fix would be giving precedence to spark property (which might be correct in most cases when it was set).
PySpark RowMatrix Constructor Has Type Erasure Issue	Implementing tallSkinnyQR in SPARK-9656 uncovered a bug with our PySpark RowMatrix constructor. As discussed on the dev list here, there appears to be an issue with type erasure with RDDs coming from Java, and by extension from PySpark. Although we are attempting to construct a RowMatrix from an RDD[Vector] in PythonMLlibAPI, the Vector type is erased, resulting in an RDD[Object]. Thus, when calling Scala's tallSkinnyQR from PySpark, we get a Java ClassCastException in which an Object cannot be cast to a Spark Vector. As noted in the aforementioned dev list thread, this issue was also encountered with DecisionTrees, and the fix involved an explicit retag of the RDD with a Vector type. Thus, this PR will apply that fix to the createRowMatrix helper function in PythonMLlibAPI. IndexedRowMatrix and CoordinateMatrix do not appear to have this issue likely due to their related helper functions in PythonMLlibAPI creating the RDDs explicitly from DataFrames with pattern matching, thus preserving the types. The following reproduces this issue on the latest Git head, 1.5.1, and 1.5.0: from pyspark.mllib.linalg.distributed import RowMatrix rows = sc.parallelize([[3, -6], [4, -8], [0, 1]]) mat = RowMatrix(rows) mat._java_matrix_wrapper.call("tallSkinnyQR", True) Should result in the following exception: java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.mllib.linalg.Vector;
Not deterministic order of columns when using merging schemas.	When executing sqlContext.read.option("mergeSchema", "true").parquet(pathOne, pathTwo).printSchema() The order of columns is not deterministic, showing up in a different order sometimes. This is because of FileStatusCache in HadoopFsRelation (which ParquetRelation extends as you know). When FileStatusCache.listLeafFiles() is called, this returns Set[FileStatus] which messes up the order of Array[FileStatus]. So, after retrieving the list of leaf files including _metadata and _common_metadata, this starts to merge (separately and if necessary) the Set s of _metadata, _common_metadata and part-files in ParquetRelation.mergeSchemasInParallel(), which ends up in the different column order having the leading columns (of the first file) which the other files do not have. I think this can be resolved by using LinkedHashSet. in a simple view, If A file has 1,2,3 fields, and B file column 3,4,5, we can not ensure which column shows first since It is not deterministic. 1. Read file list (A and B) 2. Not deterministic order of (A and B or B and A) as I said. 3. It merges by reduceOption with retrieved schemas of (A and B or B and A), (which maybe also should be reduceOptionRight or reduceOptionLeft). 4. The output columns would be 1,2,3,4,5 when A and B, or 3.4.5.1.2 when B and A.
spark.rpc config not propagated to executors	spark.rpc conf doesn't get propagated to executors because RpcEnv.create is done before properties are fetched from the driver.
Creating an InputDStream but not using it throws NPE	If an InputDStream is not used, its rememberDuration will null and DStreamGraph.getMaxInputStreamRememberDuration will throw NPE.
input_file_name() returns "" for external tables	Given an external table definition where the data consists of many CSV files, input_file_name() returns empty strings. Table definition: CREATE EXTERNAL TABLE external_test(page_id INT, impressions INT)  ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES (    "separatorChar" = ",",    "quoteChar"     = "\"",    "escapeChar"    = "\\" )   LOCATION 'file:///Users/sim/spark/test/external_test' Query: sql("SELECT input_file_name() as file FROM external_test").show Output: +----+ |file| +----+ |    | |    | ... |    | +----+
hour/minute/second returns negative value	Chip Sands added a comment - 3 hours ago Using Spark Thrift Server hour( '1961-08-30 06:06:10') is returning (-17) for the hour in 1.5 and 1.5.1
glm does not work with long formula	Because deparse() will break the long string into multiple lines.
sqlContext doesn't use PathFilter	When sqlContext reads JSON files, it doesn't use PathFilter in the underlying SparkContext val sc = new SparkContext(conf) sc.hadoopConfiguration.setClass("mapreduce.input.pathFilter.class", classOf[TmpFileFilter], classOf[PathFilter]) val sqlContext = new org.apache.spark.sql.SQLContext(sc) The definition of TmpFileFilter is: TmpFileFilter.scala import org.apache.hadoop.fs.{Path, PathFilter}  class TmpFileFilter  extends PathFilter {   override def accept(path : Path): Boolean = !path.getName.endsWith(".tmp") } When use sqlContext to read JSON files, e.g., sqlContext.read.schema(mySchema).json(s3Path), Spark will throw out an exception: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: s3://chef-logstash-access-backup/2015/10/21/00/logstash-172.18.68.59-s3.1445388158944.gz.tmp It seems sqlContext can see .tmp files while sc can not, which causes the above exception
row.getInt(i) if row[i]=null returns 0	row.getInt|Float|Double in SPARK RDD return 0 if row[index] is null. (Even according to the document they should throw nullException error)
spark on yarn spark-class --num-workers doesn't work	Using the old spark-class and --num-workers interface, --num-workers parameter is ignored and always uses default number of executors (2). bin/spark-class org.apache.spark.deploy.yarn.Client --jar lib/spark-examples-1.5.2.0-hadoop2.6.0.16.1506060127.jar --class org.apache.spark.examples.SparkPi --num-workers 4 --worker-memory 2g --master-memory 1g --worker-cores 1 --queue default
pmml version attribute missing in the root node	The current pmml models generated do not specify the pmml version in its root node. This is a problem when using this pmml model in other tools because they expect the version attribute to be set explicitly.
SQL execution very slow for nested query plans because of DataFrame.withNewExecutionId	For nested query plans like a recursive unionAll, withExecutionId is extremely slow, likely because of repeated string concatenation in QueryPlan.simpleString Test case: (1 to 100).foldLeft[Option[DataFrame]] (None) { (curr, idx) =>     println(s"PROCESSING >>>>>>>>>>> $idx")     val df = sqlContext.sparkContext.parallelize((0 to 10).zipWithIndex).toDF("A", "B")     val union = curr.map(_.unionAll(df)).getOrElse(df)     union.cache()     println(">>" + union.count)     //union.show()     Some(union)   } Stack trace: scala.collection.TraversableOnce$class.addString(TraversableOnce.scala:320) scala.collection.AbstractIterator.addString(Iterator.scala:1157) scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:286) scala.collection.AbstractIterator.mkString(Iterator.scala:1157) scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:288) scala.collection.AbstractIterator.mkString(Iterator.scala:1157) org.apache.spark.sql.catalyst.trees.TreeNode.argString(TreeNode.scala:364) org.apache.spark.sql.catalyst.trees.TreeNode.simpleString(TreeNode.scala:367) org.apache.spark.sql.catalyst.plans.QueryPlan.simpleString(QueryPlan.scala:168) org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:401) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$1.apply(TreeNode.scala:403) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$1.apply(TreeNode.scala:403) scala.collection.immutable.List.foreach(List.scala:318) org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:403) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$1.apply(TreeNode.scala:403) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$1.apply(TreeNode.scala:403) scala.collection.immutable.List.foreach(List.scala:318) org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:403) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$1.apply(TreeNode.scala:403) org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$generateTreeString$1.apply(TreeNode.scala:403) scala.collection.immutable.List.foreach(List.scala:318) org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:403) org.apache.spark.sql.catalyst.trees.TreeNode.treeString(TreeNode.scala:372) org.apache.spark.sql.catalyst.trees.TreeNode.toString(TreeNode.scala:369) org.apache.spark.sql.SQLContext$QueryExecution.stringOrError(SQLContext.scala:936) org.apache.spark.sql.SQLContext$QueryExecution.toString(SQLContext.scala:949) org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:52) org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:1903) org.apache.spark.sql.DataFrame.collect(DataFrame.scala:1384) org.apache.spark.sql.DataFrame.count(DataFrame.scala:1402)
serde parameters should be set only when all params are ready	see HIVE-7975 and HIVE-12373 With changed semantic of setters in thrift objects in hive, setter should be called only after all parameters are set. It's not problem of current state but will be a problem in some day.
ml.feature.Word2Vec.transform() should not recompute word-vector map each time	org.apache.spark.ml.feature.Word2Vec.transform() very slow. we should not read broadcast every sentence.
inserting date with leading zero inserts null example '0001-12-10'	inserting date with leading zero inserts null value, example '0001-12-10'. This worked until 1.5/1.5.1
Update dynamic allocation docs to reflect supported cluster managers	It still says it's only supported on YARN. In reality it is supported on all coarse-grained modes now.
Fix incorrect kryo buffer default value in docs	The default is 64K, but the doc says it's 2? https://spark.apache.org/docs/1.5.0/tuning.html#data-serialization If your objects are large, you may also need to increase the spark.kryoserializer.buffer config property. The default is 2, but this value needs to be large enough to hold the largest object you will serialize.
Example for sqlContext.createDataDrame from pandas.DataFrame has a typo	PySpark documentation error: sqlContext.createDataFrame(pandas.DataFrame([[1, 2]]).collect())  Results in: --------------------------------------------------------------------------- AttributeError                            Traceback (most recent call last) <ipython-input-14-120201b3ef75> in <module>() ----> 1 sqlContext.createDataFrame(pandas.DataFrame([[1, 2]]).collect())  /usr/local/src/bluemix_ipythonspark_141/notebook/lib/python2.7/site-packages/pandas-0.14.0-py2.7-linux-x86_64.egg/pandas/core/generic.pyc in __getattr__(self, name)    1841                 return self[name]    1842             raise AttributeError("'%s' object has no attribute '%s'" % -> 1843                                  (type(self).__name__, name))    1844     1845     def __setattr__(self, name, value):  AttributeError: 'DataFrame' object has no attribute 'collect'
Word2Vec code failed compile in Scala 2.11	https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Compile/job/Spark-Master-Scala211-Compile/2007/consoleFull [error] [warn] /home/jenkins/workspace/Spark-Master-Scala211-Compile/mllib/src/main/scala/org/apache/spark/ml/feature/Word2Vec.scala:149: no valid targets for annotation on value wordVectors - it is discarded unused. You may specify targets with meta-annotations, e.g. @(transient @param) [error] [warn]     @transient wordVectors: feature.Word2VecModel) [error] [warn] 
When invoking method " apply(fields: java.util.List[StructField])" in "StructType", get exception "java.lang.ClassCastException:	When invoking method " apply(fields: java.util.List[StructField])" in Object "StructType", get exception "java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.sql.types.StructField;  at org.apache.spark.sql.types.StructType$.apply"
UDFRegistration Drops Input Type Information	The UserDefinedFunction returned by the UDFRegistration does not contain the input type information, although that information is available. To fix the issue the last line of every register function would had to be changed to "UserDefinedFunction(func, dataType, inputType)" or is there any specific reason this was not done?
Casting integer types to timestamp has unexpected semantics	Casting from integer types to timestamp treats the source int as being in millis. Casting from timestamp to integer types creates the result in seconds. This leads to behavior like: scala> sql("select cast(cast (1234 as timestamp) as bigint)").show +---+ |_c0| +---+ |  1| +---+ Double's on the other hand treat it as seconds when casting to and from: scala> sql("select cast(cast (1234.5 as timestamp) as double)").show +------+ |   _c0| +------+ |1234.5| +------+ This also breaks some other functions which return long in seconds, in particular, unix_timestamp. scala> sql("select cast(unix_timestamp() as timestamp)").show +--------------------+ |                 _c0| +--------------------+ |1970-01-17 10:03:...| +--------------------+  scala> sql("select cast(unix_timestamp() *1000 as timestamp)").show +--------------------+ |                 _c0| +--------------------+ |2015-11-12 23:26:...| +--------------------+
Let UDF to handle null value	I notice that currently spark will take the long field as -1 if it is null. Here's the sample code. sqlContext.udf.register("f", (x:Int)=>x+1) df.withColumn("age2", expr("f(age)")).show()  //////////////// Output /////////////////////// +----+-------+----+ | age|   name|age2| +----+-------+----+ |null|Michael|   0| |  30|   Andy|  31| |  19| Justin|  20| +----+-------+----+ I think for the null value we have 3 options Use a special value to represent it (what spark does now) Always return null if the udf input has null value argument Let udf itself to handle null I would prefer the third option
Legacy Netty-RPC based submission in standalone mode does not work	When the application is to be submitted in cluster mode and standalone Spark scheduler is used either legacy RPC based protocol or REST based protocol can be used. Spark submit firstly tries REST and if it fails it tries RPC. When Akka based RPC is used, the REST based connection fails immediately because Akka rejects non-Akka connection. However in Netty based RPC, the REST client seems to wait for the response indefinitely, thus making it impossible to fail and try RPC. The fix is quite simple - set a timeout on reading response from the server.
bin/pyspark --version doesn't return version and exit	bin/pyspark --help offers a --version option: $ ./spark/bin/pyspark --help Usage: ./bin/pyspark [options]  Options: ...   --version,                  Print the version of current Spark ... However, trying to get the version in this way doesn't yield the expected results. Instead of printing the version and exiting, we get the version, a stack trace, and then get dropped into a broken PySpark shell. $ ./spark/bin/pyspark --version Python 2.7.10 (default, Aug 11 2015, 23:39:10)  [GCC 4.8.3 20140911 (Red Hat 4.8.3-9)] on linux2 Type "help", "copyright", "credits" or "license" for more information. Welcome to       ____              __      / __/__  ___ _____/ /__     _\ \/ _ \/ _ `/ __/  '_/    /___/ .__/\_,_/_/ /_/\_\   version 1.5.2       /_/                          Type --help for more information. Traceback (most recent call last):   File "/home/ec2-user/spark/python/pyspark/shell.py", line 43, in <module>     sc = SparkContext(pyFiles=add_files)   File "/home/ec2-user/spark/python/pyspark/context.py", line 110, in __init__     SparkContext._ensure_initialized(self, gateway=gateway)   File "/home/ec2-user/spark/python/pyspark/context.py", line 234, in _ensure_initialized     SparkContext._gateway = gateway or launch_gateway()   File "/home/ec2-user/spark/python/pyspark/java_gateway.py", line 94, in launch_gateway     raise Exception("Java gateway process exited before sending the driver its port number") Exception: Java gateway process exited before sending the driver its port number >>>  >>> sc Traceback (most recent call last):   File "<stdin>", line 1, in <module> NameError: name 'sc' is not defined
Duplicate creating the RDD in file stream when recovering from checkpoint data	I have a case to monitor a HDFS folder, then enrich the incoming data from the HDFS folder via different table (about 15 reference tables) and send to different hive table after some operations. The code is as this: val txt = ssc.textFileStream(folder).map(toKeyValuePair).reduceByKey(removeDuplicates) val refTable1 = ssc.textFileStream(refSource1).map(parse(_)).updateStateByKey(...) txt.join(refTable1).map(..).reduceByKey(...).foreachRDD(   rdd => {      // insert into hive table   } )  val refTable2 = ssc.textFileStream(refSource2).map(parse(_)).updateStateByKey(...) txt.join(refTable2).map(..).reduceByKey(...).foreachRDD(   rdd => {      // insert into hive table   } )  /// more refTables in following code The batchInterval of this application is set to 30 seconds, the checkpoint interval is set to 10 minutes, every batch in txt has 60 files After recovered from checkpoint data, I can see lots of log to create the RDD in file stream: rdd in each batch of file stream was been recreated 15 times, and it takes about 5 minutes to create so much file RDD. During this period, 10K+ broadcast had been created and almost used all the block manager space. After some investigation, we found that the DStream.restoreCheckpointData would be invoked at each output (DStream.foreachRDD in this case), and no flag to indicate that this DStream had been restored, so the RDD in file stream was been recreated. Suggest to add on flag to control the restore process to avoid the duplicated work.
SparkR can not output help information for SparkR:::predict	R users often get help information for a method like this: > ?predict or > help(predict) For SparkR we should provide the help information for the SparkR package and base R package(usually stats package). But for "predict" method, it can only output the help information from package:stats.
Prevent the call to StreamingContext#stop() in the listener bus's thread	Quoting Shixiong's comment from https://github.com/apache/spark/pull/9723 : The user should not call stop or other long-time work in a listener since it will block the listener thread, and prevent from stopping SparkContext/StreamingContext.  I cannot see an approach since we need to stop the listener bus's thread before stopping SparkContext/StreamingContext totally. Proposed solution is to prevent the call to StreamingContext#stop() in the listener bus's thread
When deployed against remote Hive metastore, HiveContext.executionHive points to wrong metastore	When using remote metastore, execution Hive client somehow is initialized to point to the actual remote metastore instead of the dummy local Derby metastore. To reproduce this issue: Configuring conf/hive-site.xml to point to a remote Hive 1.2.1 metastore. Set hive.metastore.uris to thrift://localhost:9083. Start metastore service using $HIVE_HOME/bin/hive --service metastore Start Thrift server with remote debugging options Attach the debugger to the Thrift server driver process, we can verify that executionHive points to the remote metastore rather than the local execution Derby metastore.
Using java.sql.Timestamp and java.sql.Date in where clauses on JDBC dataframes causes SQLServerException	I have a MSSQL table that has a timestamp column and am reading it using DataFrameReader.jdbc. Adding a where clause which compares a timestamp range causes a SQLServerException. The problem is in https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala#L264 (compileValue) which should surround timestamps/dates with quotes (only does it for strings). Sample pseudo-code: val beg = new java.sql.Timestamp(...) val end = new java.sql.Timestamp(...) val filtered = jdbcdf.where($"TIMESTAMP_COLUMN" >= beg && $"TIMESTAMP_COLUMN" < end) Generated SQL query: "TIMESTAMP_COLUMN >= 2015-01-01 00:00:00.0" Query should use quotes around timestamp: "TIMESTAMP_COLUMN >= '2015-01-01 00:00:00.0'" Fallback is to filter client-side which is extremely inefficient as the whole table needs to be downloaded to each Spark executor. Thanks
Flaky test: KafkaStreamTests.test_kafka_direct_stream_foreach_get_offsetRanges	Jenkins link: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/46041/consoleFull ====================================================================== ERROR: test_kafka_direct_stream_foreach_get_offsetRanges (__main__.KafkaStreamTests) Test the Python direct Kafka stream foreachRDD get offsetRanges. ---------------------------------------------------------------------- Traceback (most recent call last):   File "/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/streaming/tests.py", line 876, in setUp     self._kafkaTestUtils.setup()   File "/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py", line 813, in __call__     answer, self.gateway_client, self.target_id, self.name)   File "/home/jenkins/workspace/SparkPullRequestBuilder/python/lib/py4j-0.9-src.zip/py4j/protocol.py", line 308, in get_return_value     format(target_id, ".", name), value) Py4JJavaError: An error occurred while calling o11914.setup. : org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 6000  at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:880)  at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)  at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)  at org.apache.spark.streaming.kafka.KafkaTestUtils.setupEmbeddedZookeeper(KafkaTestUtils.scala:99)  at org.apache.spark.streaming.kafka.KafkaTestUtils.setup(KafkaTestUtils.scala:122)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)  at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)  at py4j.Gateway.invoke(Gateway.java:259)  at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)  at py4j.commands.CallCommand.execute(CallCommand.java:79)  at py4j.GatewayConnection.run(GatewayConnection.java:209)  at java.lang.Thread.run(Thread.java:745)
pyspark reduceByKeyAndWindow does not handle unspecified invFunc (invFunc=None)	invFunc is optional and can be None. Instead of invFunc (the parameter) invReduceFunc (a local function) was checked for trueness (that is, not None, in this context). A local function is never None, thus the case of invFunc=none (a common one when inverse reduction is not defined) is treated incorrectly, resulting in loss of data.
insert of timestamp with factional seconds inserts a NULL	Using the Thrift jdbc interface. The insert of the value of "1970-01-01 00:00:00.123456789" to a timestamp column, inserts a NULL into the database. I am aware the of the change  From 1.5 releases notes Timestamp Type¡¯s precision is reduced to 1 microseconds (1us). However, to be compatible with previous versions, I would suggest either rounding or truncating the fractional seconds not inserting a NULL.
ExecutorClassLoader cannot see any resources from parent class loader	This issue starts from finding root reason from strange problem from spark-shell (and zeppelin) which is not a problem for spark-submit. https://mail-archives.apache.org/mod_mbox/spark-user/201511.mbox/%3CCAF5108jMXyOjiGmCgr%3Ds%2BNvTMcyKWMBVM1GsrH7Pz4xUj48LfA%40mail.gmail.com%3E After some hours (over days) digging into the detail, I found that ExecutorClassLoader cannot see any resource files which can be seen from parent class loader. ExecutorClassLoader itself doesn't need to lookup resource files cause REPL doesn't generate these, but it should delegate lookup to parent class loader. I'll provide the pull request which includes tests which fails on master soon.
Spark-Hive doesn't work with Kerberos in local mode	There is an error between Spark-hive and Hive on a Hadoop cluster. I have found the issue recently fixed: SPARK-10181 , but it doesn't work in local mode. I think it's enough to add an "if" to that patch or make the location of a keytab setuped for all environments. stacktrace: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)] at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271) at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52) at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:422) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024) at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234) at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174) at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503) at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:193) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:422) at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:183) at org.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:179) at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:228) at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:187) at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:394) at org.apache.spark.sql.hive.HiveContext.defaultOverrides(HiveContext.scala:176) at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:179) ... Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt) at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147) at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122) at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187) at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224) at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212) at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ... 52 more
DAGScheduler source registered too early with MetricsSystem	I see this log message when starting apps on YARN: 15/11/18 13:12:56 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set. That's because DAGScheduler registers itself with the metrics system in its constructor, and the DAGScheduler is instantiated before "spark.app.id" is set in the context's SparkConf.
AkkaRpcEnvSuite is prone to port-contention-related flakiness	The AkkaRpcEnvSuite tests appear to be prone to port-contention-related flakiness in Jenkins: Error Message  Failed to bind to: localhost/127.0.0.1:12362: Service 'test' failed after 16 retries! Stacktrace        java.net.BindException: Failed to bind to: localhost/127.0.0.1:12362: Service 'test' failed after 16 retries!       at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)       at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393)       at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389)       at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)       at scala.util.Try$.apply(Try.scala:161)       at scala.util.Success.map(Try.scala:206)       at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)       at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)       at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)       at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:67)       at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:82)       at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:59)       at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:59)       at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)       at akka.dispatch.BatchingExecutor$Batch.run(BatchingExecutor.scala:58)       at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41)       at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)       at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)       at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)       at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)       at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Test/job/Spark-Master-Maven-pre-YARN/4819/HADOOP_VERSION=1.2.1,label=spark-test/testReport/junit/org.apache.spark.rpc.akka/AkkaRpcEnvSuite/uriOf__ssl/ We should probably refactor these tests to not depend on a fixed port.
Register a Python function creates a new SQLContext	You can try it with sqlContext.registerFunction("stringLengthString", lambda x: len)
spark_ec2.py breaks with python3 and m3 instances	The `spark_ec2.py` script breaks when launching an m3 instance with python3 because `string.letters` is for python2 only. For python3 `string.ascii_letters` should be used instead. The PR for fixing this is here: https://github.com/apache/spark/pull/9797
Analyzer should replace current_date and current_timestamp with literals	We currently rely on the optimizer's constant folding to replace current_timestamp and current_date. However, this can still result in different values for different instances of current_timestamp/current_date if the optimizer is not running fast enough. A better solution is to replace these functions in the analyzer in one shot.
Regex for master URL for Mesos accepts incorrect zk:// prefix	val MESOS_REGEX = """(mesos|zk)://.*""".r (see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L2748) accepts "zk://..." as the master URL for Mesos. scala> val MESOS_REGEX = """(mesos|zk)://.*""".r MESOS_REGEX: scala.util.matching.Regex = (mesos|zk)://.*  scala> "zk://aaa" match { case mesosUrl @ MESOS_REGEX(_) => println(mesosUrl) } zk://aaa However, zk://localhost:5050 master URL breaks spark-shell: ?  spark git:(master) ? ./bin/spark-shell --master zk://localhost:5050 Error: Master must start with yarn, spark, mesos, or local Run with --help for usage help or --verbose for debug output It should probably be mesos://zk://localhost:5050.
Unable to resolve order by if it contains mixture of aliases and real columns.	Analyzer is unable to resolve order by if the columns in the order by clause contains a mixture of alias and real column names. Example : var var3 = sqlContext.sql("select c1 as a, c2 as b from inttab group by c1, c2 order by b, c1") This used to work in 1.4 and is failing starting 1.5 and is affecting some tpcds queries (19, 55,71)
Auto-connection close on idle channel can race with TransportClientFactory.createClient()	The recent code added in SPARK-11252 introduced a race, where an idle channel might be closed after it's returned from a TransportClientFactory.createClient() call, which would cause applications to see failures. TransportClientFactory.createClient() should reset the channel's timeout and ensure that the channel it's returning hasn't timed out.
Prevent the call to SparkContext#stop() in the listener bus's thread	This is continuation of SPARK-11761 Andrew suggested adding this protection. See tail of PR #9741
Clear spark.sql.TungstenAggregate.testFallbackStartsAt conf at end of TungstenAggregationQueryWithControlledFallbackSuite	The spark.sql.TungstenAggregate.testFallbackStartsAt configuration needs to be cleared / reset in TungstenAggregationQueryWithControlledFallbackSuite. This state cleanup was accidentally removed when removing the ability to disable Tungsten mode and the leaked configuration led to failures in subsequent tests by causing data to spill way too frequently.
Close PersistenceEngine at end of tests	In PersistenceEngineSuite, we do not call close() on the PersistenceEngine at the end of the test. For the ZooKeeperPersistenceEngine, this causes us to leak a ZooKeeperClient, causing the test logs to be spammed with connection error messages from that client: 15/11/20 05:13:35.789 pool-1-thread-1-ScalaTest-running-PersistenceEngineSuite-SendThread(localhost:15741) INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:15741. Will not attempt to authenticate using SASL (unknown error) 15/11/20 05:13:35.790 pool-1-thread-1-ScalaTest-running-PersistenceEngineSuite-SendThread(localhost:15741) WARN ClientCnxn: Session 0x15124ff48dd0000 for server null, unexpected error, closing socket connection and attempting reconnect java.net.ConnectException: Connection refused  at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)  at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)  at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)  at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)
Speculation Tasks Cause ProgressBar UI Overflow	When there are speculative tasks in stage, the started tasks + completed tasks can be greater than total number of tasks. It leads to the started progress block to overflow to next line. Visually the light blue progress block becomes no longer visible when it happens. The fix should be as trivial as to cap the number of started task by total - completed task. https://github.com/apache/spark/blob/branch-1.6/core/src/main/scala/org/apache/spark/ui/UIUtils.scala#L322
Streaming programming guide references wrong dependency version	SPARK-11245 have upgraded twitter dependency to 4.0.4
SQL UI does not work with PySpark	
Filter pushdown does not work with aggregation with alias	
Not all the temp dirs had been deleted when the JVM exits	deleting the temp dir like that val a = mutable.Set(1,2,3,4,7,0,98,9,8) a.foreach(x => { a.remove(x) }) You may not modify a collection while traversing or iterating over it.
Spark JDBC write only works on techologies with transaction support	Writing DataFrames out to a JDBC destination currently requires the JDBC driver/ database to support transaction. This is because spark/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala always calls commit()/rollback(). Some technologies do not support transactions and their drivers will throw an exception if commit()/rollback() is used. For these technologies (like Progress JDBC Driver for Cassandra) this is a blocking problem. Prior to using transaction support JdbcUtils.scala needs to check whether the drivers does support transaction. Check can be done via conn.getMetaData().supportsDataManipulationTransactionsOnly() || conn.getMetaData().supportsDataDefinitionAndDataManipulationTransactions() The working code change can be seen here 
Executor thread dump is broken	The driver needs to know the executor listening address to send the thread dump request. However, in Netty RPC, the executor doesn't listen to any port, and the executor thread dump feature is broken. Here is the stack trace: 15/11/25 18:32:20 WARN NettyRpcEndpointRef: Error sending message [message = GetRpcHostPortForExecutor(0)] in 1 attempts java.lang.NullPointerException  at org.apache.spark.storage.BlockManagerMasterEndpoint$$$$$$9c99c11fa524b2acd17a791a15f5b988$$$$$getRpcHostPortForExecutor$1$$anonfun$apply$3.apply(BlockManagerMasterEndpoint.scala:399)  at org.apache.spark.storage.BlockManagerMasterEndpoint$$$$$$9c99c11fa524b2acd17a791a15f5b988$$$$$getRpcHostPortForExecutor$1$$anonfun$apply$3.apply(BlockManagerMasterEndpoint.scala:397)  at scala.Option.map(Option.scala:145)  at org.apache.spark.storage.BlockManagerMasterEndpoint$$$$$$9c99c11fa524b2acd17a791a15f5b988$$$$$getRpcHostPortForExecutor$1.apply(BlockManagerMasterEndpoint.scala:397)  at org.apache.spark.storage.BlockManagerMasterEndpoint$$$$$$9c99c11fa524b2acd17a791a15f5b988$$$$$getRpcHostPortForExecutor$1.apply(BlockManagerMasterEndpoint.scala:396)  at scala.Option.flatMap(Option.scala:170)  at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$getRpcHostPortForExecutor(BlockManagerMasterEndpoint.scala:396)  at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:79)  at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:103)  at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:206)  at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:97)  at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:224)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745) This issue doesn't appear in `local` mode, because the driver thread dump has a different code path
Branch 1.6's hadoop 2.2 tests always fail the entire VersionsSuite	Right now, we hardcoded hadoop 2.4 to download when we need to download hive and hadoop jars for metadata hive. However, SPARK-9596 makes isoalted client loader share hadoop classes. So, if there is any compatibility issues between the shared hadoop and hadoop 2.4, VersionsSuite will failed. Actually, in spark 1.6's hadoop 2.2 tests, VersionsSuite always fail (https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Dashboard%20(Patrick)/job/Spark-1.6-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.2,label=spark-test/40/testReport/junit/org.apache.spark.sql.hive.client/). Because HftpFileSystem has been moved to a new location in Hadoop 2.3+. From Hadoop 2.4, we get org.apache.hadoop.hdfs.web.HftpFileSystem. But, when we look up the class (by looking at hadoop 2.2's jars), it is not in this location.
offsetRanges attribute missing in Kafka RDD when resuming from checkpoint	SPARK-8389 added offsetRanges to Kafka direct streams. And SPARK-10122 fixed the issue of not ending up with non-Kafka RDDs when chaining transforms to Kafka RDDs. It appears that this issue remains for the case where a streaming application using Kafka direct streams is initialized from the checkpoint directory. The following is a representative example where everything works as expected during the first run, but exceptions are thrown on a subsequent run when the context is being initialized from the checkpoint directory. test_checkpoint.py from pyspark import SparkContext                                                                                             from pyspark.streaming import StreamingContext                                                                               from pyspark.streaming.kafka import KafkaUtils                                                                                 def attach_kafka_metadata(kafka_rdd):                                                                                            offset_ranges = kafka_rdd.offsetRanges()                                                                                                                                                                                                                  return kafka_rdd                                                                                                                                                                                                                                                                                                                                                                   def create_context():                                                                                                            sc = SparkContext(appName='kafka-test')                                                                                      ssc = StreamingContext(sc, 10)                                                                                               ssc.checkpoint(CHECKPOINT_URI)                                                                                                                                                                                                                            kafka_stream = KafkaUtils.createDirectStream(                                                                                    ssc,                                                                                                                         [TOPIC],                                                                                                                     kafkaParams={                                                                                                                    'metadata.broker.list': BROKERS,                                                                                         },                                                                                                                       )                                                                                                                            kafka_stream.transform(attach_kafka_metadata).count().pprint()                                                                                                                                                                                            return ssc                                                                                                                                                                                                                                                                                                                                                                         if __name__ == "__main__":                                                                                                       ssc = StreamingContext.getOrCreate(CHECKPOINT_URI, create_context)                                                           ssc.start()                                                                                                                  ssc.awaitTermination() Exception on resuming from checkpoint Traceback (most recent call last):   File "/home/spark/spark/python/lib/pyspark.zip/pyspark/streaming/util.py", line 62, in call     r = self.func(t, *rdds)   File "/home/spark/spark/python/lib/pyspark.zip/pyspark/streaming/kafka.py", line 344, in <lambda>   File "/home/spark/batch/test_checkpoint.py", line 12, in attach_kafka_metadata     offset_ranges = kafka_rdd.offsetRanges() AttributeError: 'RDD' object has no attribute 'offsetRanges'
RDD checkpointing does not preserve partitioner	
SparkR.init does not support character vector for sparkJars and sparkPackages	https://spark.apache.org/docs/1.5.2/api/R/sparkR.init.html The example says initial the sparkJars variable with sparkJars=c("jarfile1.jar","jarfile2.jar") But when I try this in Rstudio, it actually gave me a warning: Warning message: In if (jars != "") { : the condition has length > 1 and only the first element will be used and you can see in the logs : Launching java with spark-submit command /home/liushiqi9/dev/data-science/tools/phemi-spark-zeppelin-client/phemi/phemi-spark/spark/bin/spark-submit --jars /home/liushiqi9/dev/data-science/tools/phemi-spark-zeppelin-client/phemi/libs/phemi-datasource.jar sparkr-shell /tmp/RtmpThLAQn/backend_port39cd33f76fcd --jars /home/liushiqi9/dev/data-science/tools/phemi-spark-zeppelin-client/phemi/libs/phemi-spark-lib-1.0-all.jar sparkr-shell /tmp/RtmpThLAQn/backend_port39cd33f76fcd So it's try to upload this two jars into two different shell I think. And in the spark UI environment page I only see the first jar. The right way to do it is: sparkJars=c("jarfile1.jar,jarfile2.jar")
Fishy test of "don't call ssc.stop in listener"	I just noticed that some of the tests blocked at the case "don't call ssc.stop in listener" in StreamingListenerSuite https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/46766/console https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/46776/console https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/46774/console The PRs corresponding to the tests are on different things...I think something fishy hidden in the case...
[SQL] get_json_object is unable to return a correct result for null literals	So far, our get_json_object returns the same results of output for the following two cases. Both results are "null": val tuple: Seq[(String, String)] = ("5", """ {"f1": null} """) :: Nil val df: DataFrame = tuple.toDF("key", "jstring") val res = df.select(functions.get_json_object($"jstring", "$.f1")).collect() val tuple2: Seq[(String, String)] = ("5", """ {"f1": "null"} """) :: Nil val df2: DataFrame = tuple2.toDF("key", "jstring") val res3 = df2.select(functions.get_json_object($"jstring", "$.f1")).collect()
Incorrect results when aggregate joined data	I have following issue. I created 2 dataframes from JDBC (MySQL) and joined them (t1 has fk1 to t2) t1 = sqlCtx.read.jdbc("jdbc:mysql://XXX", t1, id1, 0, size1, 200).cache() t2 = sqlCtx.read.jdbc("jdbc:mysql://XXX", t2).cache() joined = t1.join(t2, t1.fk1 == t2.id2, "left_outer") Important: both table are cached, so results should be the same on every query. Then I did come counts: t1.count() -> 5900729 t1.registerTempTable("t1") sqlCtx.sql("select distinct(id1) from t1").count() -> 5900729 t2.count() -> 54298 joined.count() -> 5900729 And here magic begins - I counted distinct id1 from joined table joined.registerTempTable("joined") sqlCtx.sql("select distinct(id1) from joined").count() Results varies (are different on every run) between 5899000 and  5900000 but never are equal to 5900729. In addition. I did more queries: sqlCtx.sql("select id1, count(*) from joined group by id1 having count(*) > 1").collect()  This gives some results but this query return 1 len(sqlCtx.sql("select * from joined where id1 = result").collect()) What's wrong ?
Integer overflow when do sampling.	In my case, some partitions contain too much items. When do range partition, exception thrown as: java.lang.IllegalArgumentException: n must be positive at java.util.Random.nextInt(Random.java:300) at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:58) at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259) at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$18.apply(RDD.scala:703) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$18.apply(RDD.scala:703) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63) at org.apache.spark.scheduler.Task.run(Task.scala:70) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)
Executors use heartbeatReceiverRef to report heartbeats and task metrics that might not be initialized and leads to NullPointerException	When Executor starts it starts driver heartbeater (using startDriverHeartbeater()) that uses heartbeatReceiverRef that is initialized later and there is a possibility of NullPointerException (after spark.executor.heartbeatInterval or 10s). WARN Executor: Issue communicating with driver in heartbeater java.lang.NullPointerException  at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:447)  at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:467)  at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:467)  at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:467)  at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1717)  at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:467)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745)
User JVM shutdown hook can cause deadlock at shutdown	Here's a simplification of a deadlock that can occur a shutdown if the user app has also installed a shutdown hook to clean up: Spark Shutdown Hook thread runs SparkShutdownHookManager.runAll() is invoked, locking SparkShutdownHookManager as it is synchronized A user shutdown hook thread runs User hook tries to call, for example StreamingContext.stop(), which is synchronized and locks it User hook blocks when the StreamingContext tries to remove() the Spark Streaming shutdown task, since it's synchronized per above Spark Shutdown Hook tries to execute the Spark Streaming shutdown task, but blocks on StreamingContext.stop() I think this is actually not that critical, since it requires a pretty specific setup, and I think it can be worked around in many cases by integrating with Hadoop's shutdown hook mechanism like Spark does so that these happen serially. I also think it's solvable in the code by not locking SparkShutdownHookManager in the 3 methods that are synchronized since these are really only protecting hooks. runAll() shouldn't hold the lock while executing hooks.
EventLog for completed applications always not found if spark.eventLog.compress is true	EventLog for completed applications always not found with following settings in spark-defaults.conf : spark.eventLog.enabled          true spark.eventLog.compress         true spark.eventLog.dir              file:///tmp/spark-events On Web UI, it shows always "No event logs found for application app-2015xxxxxxx" but there are files under /tmp/spark-events/ , such as: app-20151130143343-0000.snappy app-20151130144008-0001.snappy.inprogress app-20151130152905-0002.snappy.inprogress app-20151130153215-0003.snappy
NewHadoopRDD: TaskAttemptContext should be created only after calling setConf.	TaskAttemptContext's constructor will clone the configuration instead of referencing it. Calling setConf after creating TaskAttemptContext makes any changes to the configuration inside setConf unperceived by RecordReader instances.
Master rebuilding historical SparkUI should be asynchronous	When a long-running application finishes, it takes a while (sometimes minutes) to rebuild the SparkUI. However, in Master.scala this is currently done within the RPC event loop, which runs only in 1 thread. Thus, in the mean time no other applications can register with this master.
NettyBlockTransferSecuritySuite "security mismatch auth off on client" test is flaky	The NettyBlockTransferSecuritySuite "security mismatch auth off on client" test is flaky in Jenkins. Here's a link to a report that lists the latest failures over the past week+: https://spark-tests.appspot.com/tests/org.apache.spark.network.netty.NettyBlockTransferSecuritySuite/security%20mismatch%20auth%20off%20on%20client#latest-failures In all of these failures, the test failed with the following exception: Futures timed out after [1000 milliseconds]         java.util.concurrent.TimeoutException: Futures timed out after [1000 milliseconds]       at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)       at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)       at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)       at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)       at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)       at scala.concurrent.Await$.ready(package.scala:86)       at org.apache.spark.network.netty.NettyBlockTransferSecuritySuite.fetchBlock(NettyBlockTransferSecuritySuite.scala:151)       at org.apache.spark.network.netty.NettyBlockTransferSecuritySuite.org$apache$spark$network$netty$NettyBlockTransferSecuritySuite$$testConnection(NettyBlockTransferSecuritySuite.scala:116)       at org.apache.spark.network.netty.NettyBlockTransferSecuritySuite$$anonfun$5.apply$mcV$sp(NettyBlockTransferSecuritySuite.scala:90)       at org.apache.spark.network.netty.NettyBlockTransferSecuritySuite$$anonfun$5.apply(NettyBlockTransferSecuritySuite.scala:84)       at org.apache.spark.network.netty.NettyBlockTransferSecuritySuite$$anonfun$5.apply(NettyBlockTransferSecuritySuite.scala:84)       at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)       at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)       at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)       at org.scalatest.Transformer.apply(Transformer.scala:22)       at org.scalatest.Transformer.apply(Transformer.scala:20)       at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)       at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)       at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)       at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)       at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)       at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)       at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)       at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)       at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)       at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)       at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)       at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)       at scala.collection.immutable.List.foreach(List.scala:318)       at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)       at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)       at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)       at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)       at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)       at org.scalatest.Suite$class.run(Suite.scala:1424)       at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)       at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)       at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)       at org.scalatest.SuperEngine.runImpl(Engine.scala:545)       at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)       at org.scalatest.FunSuite.run(FunSuite.scala:1555)       at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)       at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)       at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)       at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)       at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)       at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)       at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)       at org.scalatest.Suite$class.run(Suite.scala:1421)       at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)       at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)       at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)       at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)       at scala.collection.immutable.List.foreach(List.scala:318)       at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)       at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)       at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)       at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)       at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)       at org.scalatest.tools.Runner$.main(Runner.scala:860)       at org.scalatest.tools.Runner.main(Runner.scala)
Coalesce does not consider shuffle in PySpark	
Fix thread pools that cannot cache tasks in Worker and AppClient	SynchronousQueue cannot cache any task. This issue is similar to SPARK-11999. It's an easy fix. Just use the fixed ThreadUtils.newDaemonCachedThreadPool.
Update spark-ec2 versions	spark-ec2's version strings are out-of-date. The latest versions of Spark need to be reflected in its internal version maps.
Recovered streaming context can sometimes run a batch twice	After recovering from checkpoint, the JobGenerator figures out which batches to run again. That can sometimes lead to a batch being submitted twice.
Cltr-C should clear current line in pyspark shell	
rddToFileName does not properly handle prefix and suffix parameters	See code here: https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala#L894 private[streaming] def rddToFileName[T](prefix: String, suffix: String, time: Time): String = { if (prefix == null) { time.milliseconds.toString } else if (suffix == null || suffix.length ==0) { prefix + "-" + time.milliseconds } else { prefix + "-" + time.milliseconds + "." + suffix } } This code does not seem to properly handle the cases where the prefix is null, but suffix is not null - the suffix should be used but is not. Also, the check for length == 0 is only applied to the suffix, bot the prefix. It seems the check should be consistent between the two. Is there a reason not to address these two issues and change the code?
[R] [SQL] Fix 'sample' functions that break R unit test cases	The existing sample functions miss the parameter 'seed', however, the corresponding function interface in `generics` has such a parameter. This could cause SparkR unit tests failed. For example, I hit it in one PR: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47213/consoleFull
stage web URI will redirect to the wrong location if it is the first URI from the application to be requested from the history server	In the history server when we open an application link for the first time, it loads the app and registers the app UI and sends a redirect to the URI that was requested. The code to send the redirect is: res.sendRedirect(res.encodeRedirectURL(req.getRequestURI())) However req.getRequestURI() is not the complete URI that was requested, it doesn't contain the query string. Stage URIs are of the following form: http://localhost:18080/history/application_1449188824095_0001/stages/stage/?id=0&attempt=0 When such a URI is the first URI from the application to be requested, it redirects to a URI like: http://localhost:18080/history/application_1449188824095_0001/stages/stage/ which errors with HTTP ERROR 400  Problem accessing /history/application_1449188824095_0001/stages/stage/. Reason:      requirement failed: Missing id parameter  Powered by Jetty:// This is not a frequent occurrence because you usually navigate to the stage URI after you have navigated to some other URI belonging to the application and then this will not happen, only when the stage URI is the first URI from the application to be requested from the history server will you see this issue.
add type coercion rule for greatest/least	
Make Utils.fetchFile support files that contain special characters	Now if a file name contains some illegal characters, such as " ", Utils.fetchFile will fail because it doesn't handle this case.
SparkR subset throw error when only set "select" argument	SparkR subset function throw error when only set "select" argument, it's easy to repreduce. In SparkR: > df <- suppressWarnings(createDataFrame(sqlContext, iris)) > subset(df, select=c("Sepal_Length", "Petal_Length", "Species")) Error in x[subset, select, ...] :    error in evaluating the argument 'i' in selecting a method for function '[': Error: argument "subset" is missing, with no default But in base R, the subset function works well with only specifying "select" argument: > df <- iris > subset(df, select=c("Sepal.Length", "Petal.Length", "Species"))     Sepal.Length Petal.Length    Species 1            5.1          1.4     setosa 2            4.9          1.4     setosa 3            4.7          1.3     setosa 4            4.6          1.5     setosa 5            5.0          1.4     setosa ......
Allow users to define a UDAF without providing details of its inputSchema	Right now, users need to provide the exact inputSchema. Otherwise, our ScalaUDAF will fail because it tries to check the schema and input arguments. We should remove this check because it is common that users may have a complex input schema type and they do not want to provide the detailed schema.
Standalone master keeps references to disassociated workers until they sent no heartbeats	While toying with Spark Standalone I've noticed the following messages in the logs of the master: INFO Master: Registering worker 192.168.1.6:59919 with 2 cores, 2.0 GB RAM INFO Master: localhost:59920 got disassociated, removing it. ... WARN Master: Removing worker-20151210090708-192.168.1.6-59919 because we got no heartbeat in 60 seconds INFO Master: Removing worker worker-20151210090708-192.168.1.6-59919 on 192.168.1.6:59919 Why does the message "WARN Master: Removing worker-20151210090708-192.168.1.6-59919 because we got no heartbeat in 60 seconds" appear when the worker should've been removed already (as pointed out in "INFO Master: localhost:59920 got disassociated, removing it.")? Could it be that the ids are different - 192.168.1.6:59919 vs localhost:59920? I started master using ./sbin/start-master.sh -h localhost and the workers ./sbin/start-slave.sh spark://localhost:7077.
No plan for BroadcastHint in some condition	Summary No plan for BroadcastHint is generated in some condition. Test Case     val df1 = Seq((1, "1"), (2, "2")).toDF("key", "value")     val parquetTempFile =       "%s/SPARK-xxxx_%d.parquet".format(System.getProperty("java.io.tmpdir"), scala.util.Random.nextInt)     df1.write.parquet(parquetTempFile)     val pf1 = sqlContext.read.parquet(parquetTempFile)     #1. df1.join(broadcast(pf1)).count()     #2. broadcast(pf1).count() Result It will trigger assertion in QueryPlanner.scala, like below: scala>     df1.join(broadcast(pf1)).count() java.lang.AssertionError: assertion failed: No plan for BroadcastHint +- Relation[key#6,value#7] ParquetRelation[hdfs://10.1.0.20:8020/tmp/SPARK-xxxx_1817830406.parquet]   at scala.Predef$.assert(Predef.scala:179)  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:59)  at org.apache.spark.sql.catalyst.planning.QueryPlanner.planLater(QueryPlanner.scala:54)  at org.apache.spark.sql.execution.SparkStrategies$BasicOperators$.apply(SparkStrategies.scala:336)  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)  at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:59)  at org.apache.spark.sql.catalyst.planning.QueryPlanner.planLater(QueryPlanner.scala:54)
Fixed potential exceptions when exiting a local cluster.	Fixed the following potential exceptions when exiting a local cluster. java.lang.AssertionError: assertion failed: executor 4 state transfer from RUNNING to RUNNING is illegal  at scala.Predef$.assert(Predef.scala:179)  at org.apache.spark.deploy.master.Master$$anonfun$receive$1.applyOrElse(Master.scala:260)  at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:116)  at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:204)  at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)  at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:215)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745) java.lang.IllegalStateException: Shutdown hooks cannot be modified during shutdown.  at org.apache.spark.util.SparkShutdownHookManager.add(ShutdownHookManager.scala:246)  at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:191)  at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:180)  at org.apache.spark.deploy.worker.ExecutorRunner.start(ExecutorRunner.scala:73)  at org.apache.spark.deploy.worker.Worker$$anonfun$receive$1.applyOrElse(Worker.scala:474)  at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:116)  at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:204)  at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)  at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:215)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745)
Mesos cluster mode is broken	The same setup worked in 1.5.2 but is now failing for 1.6.0-RC2. The driver is confused about where SPARK_HOME is. It resolves `mesos.executor.uri` or `spark.mesos.executor.home` relative to the filesystem where the driver runs, which is wrong. I1215 15:00:39.411212 28032 exec.cpp:134] Version: 0.25.0 I1215 15:00:39.413512 28037 exec.cpp:208] Executor registered on slave 130bdc39-44e7-4256-8c22-602040d337f1-S1 bin/spark-submit: line 27: /Users/dragos/workspace/Spark/dev/rc-tests/spark-1.6.0-bin-hadoop2.6/bin/spark-class: No such file or directory
Spark Streaming Java8APISuite fails in assertOrderInvariantEquals method	org.apache.spark.streaming.Java8APISuite.java is failing due to trying to sort immutable list in assertOrderInvariantEquals method. Here are the errors: Tests run: 27, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 5.948 sec <<< FAILURE! - in org.apache.spark.streaming.Java8APISuite testMap(org.apache.spark.streaming.Java8APISuite) Time elapsed: 0.217 sec <<< ERROR! java.lang.UnsupportedOperationException: null at java.util.AbstractList.set(AbstractList.java:132) at java.util.AbstractList$ListItr.set(AbstractList.java:426) at java.util.List.sort(List.java:482) at java.util.Collections.sort(Collections.java:141) at org.apache.spark.streaming.Java8APISuite.lambda$assertOrderInvariantEquals$1(Java8APISuite.java:444) testFlatMap(org.apache.spark.streaming.Java8APISuite) Time elapsed: 0.203 sec <<< ERROR! java.lang.UnsupportedOperationException: null at java.util.AbstractList.set(AbstractList.java:132) at java.util.AbstractList$ListItr.set(AbstractList.java:426) at java.util.List.sort(List.java:482) at java.util.Collections.sort(Collections.java:141) at org.apache.spark.streaming.Java8APISuite.lambda$assertOrderInvariantEquals$1(Java8APISuite.java:444) testFilter(org.apache.spark.streaming.Java8APISuite) Time elapsed: 0.209 sec <<< ERROR! java.lang.UnsupportedOperationException: null at java.util.AbstractList.set(AbstractList.java:132) at java.util.AbstractList$ListItr.set(AbstractList.java:426) at java.util.List.sort(List.java:482) at java.util.Collections.sort(Collections.java:141) at org.apache.spark.streaming.Java8APISuite.lambda$assertOrderInvariantEquals$1(Java8APISuite.java:444) testTransform(org.apache.spark.streaming.Java8APISuite) Time elapsed: 0.215 sec <<< ERROR! java.lang.UnsupportedOperationException: null at java.util.AbstractList.set(AbstractList.java:132) at java.util.AbstractList$ListItr.set(AbstractList.java:426) at java.util.List.sort(List.java:482) at java.util.Collections.sort(Collections.java:141) at org.apache.spark.streaming.Java8APISuite.lambda$assertOrderInvariantEquals$1(Java8APISuite.java:444) Results : Tests in error:  Java8APISuite.testFilter:81->assertOrderInvariantEquals:444->lambda$assertOrderInvariantEquals$1:444 ? UnsupportedOperation Java8APISuite.testFlatMap:360->assertOrderInvariantEquals:444->lambda$assertOrderInvariantEquals$1:444 ? UnsupportedOperation Java8APISuite.testMap:63->assertOrderInvariantEquals:444->lambda$assertOrderInvariantEquals$1:444 ? UnsupportedOperation Java8APISuite.testTransform:168->assertOrderInvariantEquals:444->lambda$assertOrderInvariantEquals$1:444 ? UnsupportedOperation
MLLib should use existing SQLContext instead create new one	
